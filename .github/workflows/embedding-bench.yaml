name: Embeddings Model Benchmark

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      python-version:
        description: The Python version to use
        required: false
        default: "3.12"
      client-version:
        description: The llama-stack-client-python version to test against (latest or published)
        required: false
        default: "0.2.16"
      embedding-models:
        description: Embedding models to benchmark
        required: false
        default: '["all-MiniLM-L6-v2", "granite-embedding-125m"]'
      dataset-names:
        description: The Beir datasets to test information retrieval performance
        required: false
        default: '["nfcorpus"]'
      batch-size:
        description: Batch size for document ingestion
        required: false
        default: "2"

env:
  DEFAULT_PYTHON_VERSION: "3.12"
  DEFAULT_CLIENT_VERSION: "0.2.16"
  DEFAULT_EMBEDDING_MODELS: '["all-MiniLM-L6-v2", "granite-embedding-125m"]'
  DEFAULT_DATASET_NAMES: '["nfcorpus"]'
  DEFAULT_BATCH_SIZE: "2"

jobs:
  embeddings-model-benchmark:

    runs-on: ubuntu-latest
    outputs:
      best-embedding-model: ${{ steps.benchmark.outputs.best-model }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ inputs.python-version || env.DEFAULT_PYTHON_VERSION }}
          activate-environment: true
          version: 0.7.6

      - name: Setup Ollama
        uses: ./.github/actions/setup-ollama

      - name: Install dependencies
        run: |
          uv venv .venv --python ${{ inputs.python-version || env.DEFAULT_PYTHON_VERSION }} --seed
          uv pip install -r benchmarks/beir-benchmarks/requirements.txt

          uv run --with llama-stack llama stack build --template starter --image-type venv

      - name: Run the Benchmark
        id: benchmark
        run: |
          echo "Using the following dataset names: ${{ inputs.dataset-names || env.DEFAULT_DATASET_NAMES }}"
          echo "Using the following embedding models: ${{ inputs.embedding-models || env.DEFAULT_EMBEDDING_MODELS }}"
          echo "Using the following batch size: ${{ inputs.batch-size || env.DEFAULT_BATCH_SIZE }}"

          echo "Running the Benchmark (capturing output to benchmark_results.txt)"
          # Use tee to capture output while showing logs in real-time
          DATASET_NAMES='${{ inputs.dataset-names || env.DEFAULT_DATASET_NAMES }}'
          EMBEDDING_MODELS='${{ inputs.embedding-models || env.DEFAULT_EMBEDDING_MODELS }}'
          BATCH_SIZE="${{ inputs.batch-size || env.DEFAULT_BATCH_SIZE }}"

          echo "Debug - DATASET_NAMES: $DATASET_NAMES"
          echo "Debug - EMBEDDING_MODELS: $EMBEDDING_MODELS"

          # Convert JSON arrays to space-separated values
          DATASET_ARGS=$(echo "$DATASET_NAMES" | jq -r '.[]' | tr '\n' ' ')
          EMBEDDING_ARGS=$(echo "$EMBEDDING_MODELS" | jq -r '.[]' | tr '\n' ' ')

          echo "Debug - DATASET_ARGS: $DATASET_ARGS"
          echo "Debug - EMBEDDING_ARGS: $EMBEDDING_ARGS"

          MILVUS_URL=milvus OLLAMA_URL=http://localhost:11434 uv run python beir_benchmarks.py \
            --dataset-names $DATASET_ARGS \
            --embedding-models $EMBEDDING_ARGS \
            --batch-size "$BATCH_SIZE"

        working-directory: benchmarks/beir-benchmarks

      - name: Upload Benchmark Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmarks/beir-benchmarks/results/
          retention-days: 30
