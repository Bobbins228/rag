# Secret added as example. It should be manually created with the right values via
# oc create secret generic ... before creating the llama-stack-distribution
# Important: 
#  - Make sure that the value for INFERENCE_MODEL is correct (it doesn't have points)
#  - In VLLM_URL you can use internal or external endpoints for the model. Add /v1 at the end
#  - NEVER set VLLM_TLS_VERIFY=false in production
apiVersion: v1
kind: Secret
metadata:
  name: llama-stack-inference-model-secret
type: Opaque  
stringData:
  INFERENCE_MODEL: "<your-model-id>"
  VLLM_API_TOKEN: "<paste-api-token>"
  VLLM_TLS_VERIFY: "true"                # or "false"
  VLLM_URL: "https://your-model-id.example.com/v1"
