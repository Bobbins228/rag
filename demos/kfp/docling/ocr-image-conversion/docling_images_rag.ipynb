{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the llama stack client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install llama-stack\n",
    "%pip install ragas langchain-together\n",
    "%pip install pandas\n",
    "%pip install langchain-groq\n",
    "%pip install rapidfuzz\n",
    "%pip install sacrebleu\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "vllm\n",
      "=== Available Vector Databases ===\n",
      "- ID: my_demo_image_ocr_vector_id\n",
      "  Provider: milvus\n",
      "  Embedding Model: ibm-granite/granite-embedding-125m-english\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "print(client.models.list())\n",
    "\n",
    "models = client.models.list()\n",
    "inference_llm = next((model.identifier for model in models if model.model_type == 'llm'), None)\n",
    "print(inference_llm)\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import and run the KubeFlow Pipeline\n",
    "Import the \"[docling_convert_images_pipeline_ocr_only_compiled.yaml](./docling_convert_images_pipeline_ocr_only_compiled.yaml)\" KubeFlow Pipeline into your pipeline server, then run the pipeline to insert your Image documents into the vector database.\n",
    "\n",
    "When running the pipeline, you can customize the following parameters:\n",
    "\n",
    "- `base_url`: Base URL to fetch Image files from\n",
    "- `image_filenames`: Comma-separated list of PNG/JPG/JPEG/tiff/bmp/webp filenames to download and convert\n",
    "- `num_workers`: Number of parallel workers\n",
    "- `vector_db_id`: Milvus vector database ID\n",
    "- `service_url`: Milvus service URL\n",
    "- `embed_model_id`: Embedding model to use\n",
    "- `max_tokens`: Maximum tokens per chunk\n",
    "- `use_gpu`: Enable/disable GPU acceleration\n",
    "\n",
    "Note: The compiled pipeline was generated by running `python docling_convert_images_pipeline_ocr_only.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt the LLM\n",
    "Prompt the LLM with a question in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/7680041b-0538-4440-9ff4-1ab62fce7cf5/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/7680041b-0538-4440-9ff4-1ab62fce7cf5/session/6433533b-f845-4956-8fd7-3c052f2ebe29/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 4 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': 'b563bbb7-7224-49b9-8c6d-634abc0504bf'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': 'e2cf6558-23db-4c06-8a5f-4379e9cc152a'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '01138e86-160a-4759-8c87-0b8c27e1a9b3'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '8fdb2e04-166d-4501-ab91-c3a39d034ef3'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m FAQs\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m Systems\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Enh\u001b[0m\u001b[33mancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m Service\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m Assist\u001b[0m\u001b[33mants\u001b[0m\u001b[33m:\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m Al\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-hop\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m;\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m Applications\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m Task\u001b[0m\u001b[33m Assistance\u001b[0m\u001b[33m:\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m-making\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m demonstrate\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m and\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/7680041b-0538-4440-9ff4-1ab62fce7cf5/session/6433533b-f845-4956-8fd7-3c052f2ebe29/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Describe the sequence of steps of the Ingestion Flow\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Ingestion Flow RAG sequence of steps'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 4 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': 'b563bbb7-7224-49b9-8c6d-634abc0504bf'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': 'e2cf6558-23db-4c06-8a5f-4379e9cc152a'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '01138e86-160a-4759-8c87-0b8c27e1a9b3'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '8fdb2e04-166d-4501-ab91-c3a39d034ef3'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Ingestion Flow RAG sequence of steps\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m is\u001b[0m\u001b[33m as\u001b[0m\u001b[33m follows\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m (\u001b[0m\u001b[33mChunk\u001b[0m\u001b[33ming\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Breaking\u001b[0m\u001b[33m down\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m into\u001b[0m\u001b[33m smaller\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m or\u001b[0m\u001b[33m sub\u001b[0m\u001b[33m-\u001b[0m\u001b[33mquestions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m (\u001b[0m\u001b[33mGran\u001b[0m\u001b[33mite\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Con\u001b[0m\u001b[33mverting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m into\u001b[0m\u001b[33m numerical\u001b[0m\u001b[33m representations\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m processed\u001b[0m\u001b[33m by\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m via\u001b[0m\u001b[33m v\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mv\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m (\u001b[0m\u001b[33mY\u001b[0m\u001b[33mector\u001b[0m\u001b[33m DB\u001b[0m\u001b[33m):\u001b[0m\u001b[33m St\u001b[0m\u001b[33moring\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m database\u001b[0m\u001b[33m for\u001b[0m\u001b[33m later\u001b[0m\u001b[33m use\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Top\u001b[0m\u001b[33m-K\u001b[0m\u001b[33m Ch\u001b[0m\u001b[33munks\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Select\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m top\u001b[0m\u001b[33m-k\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m that\u001b[0m\u001b[33m are\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Prompt\u001b[0m\u001b[33m Constructor\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Construct\u001b[0m\u001b[33ming\u001b[0m\u001b[33m a\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m that\u001b[0m\u001b[33m captures\u001b[0m\u001b[33m the\u001b[0m\u001b[33m essence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m (\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m-stack\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m stack\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m architecture\u001b[0m\u001b[33m to\u001b[0m\u001b[33m process\u001b[0m\u001b[33m the\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m and\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m User\u001b[0m\u001b[33m Interface\u001b[0m\u001b[33m (\u001b[0m\u001b[33mCh\u001b[0m\u001b[33mal\u001b[0m\u001b[33m VI\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Present\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m user\u001b[0m\u001b[33m-friendly\u001b[0m\u001b[33m interface\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Re\u001b[0m\u001b[33mceiving\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m and\u001b[0m\u001b[33m processing\u001b[0m\u001b[33m it\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m next\u001b[0m\u001b[33m iteration\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Con\u001b[0m\u001b[33mverting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m numerical\u001b[0m\u001b[33m representation\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m processed\u001b[0m\u001b[33m by\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m11\u001b[0m\u001b[33m.\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m In\u001b[0m\u001b[33mference\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m12\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Response\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Generating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m13\u001b[0m\u001b[33m.\u001b[0m\u001b[33m J\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRun\u001b[0m\u001b[33m/\u001b[0m\u001b[33mDebug\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Running\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m in\u001b[0m\u001b[33m J\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m for\u001b[0m\u001b[33m debugging\u001b[0m\u001b[33m and\u001b[0m\u001b[33m testing\u001b[0m\u001b[33m purposes\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m14\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m Client\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLS\u001b[0m\u001b[33m Client\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Sending\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m client\u001b[0m\u001b[33m for\u001b[0m\u001b[33m further\u001b[0m\u001b[33m processing\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m15\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Query\u001b[0m\u001b[33m Select\u001b[0m\u001b[33m W\u001b[0m\u001b[33model\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Select\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m query\u001b[0m\u001b[33m from\u001b[0m\u001b[33m the\u001b[0m\u001b[33m client\u001b[0m\u001b[33m's\u001b[0m\u001b[33m input\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m exact\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m may\u001b[0m\u001b[33m vary\u001b[0m\u001b[33m depending\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m implementation\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/agents/7680041b-0538-4440-9ff4-1ab62fce7cf5/session/6433533b-f845-4956-8fd7-3c052f2ebe29 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"my_demo_image_ocr_vector_id\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Describe the sequence of steps of the Ingestion Flow\"\n",
    "]\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"prompt>\", prompt)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=rag_agent.agent_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "We will two key metrics to show the performance of RAG server:\n",
    "1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Union\n",
    "from llama_stack_client.types.agents import Turn\n",
    "\n",
    "# This function extracts the search results for the trace of each query\n",
    "def extract_retrieved_contexts(turn_object: Turn) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack tool execution responses.\n",
    "    \n",
    "    Args:\n",
    "        turn_object: A Turn object from LlamaStack containing steps with tool responses\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_context = []\n",
    "\n",
    "    for step in turn_object.steps:\n",
    "        if step.step_type == \"tool_execution\":\n",
    "            tool_responses = step.tool_responses\n",
    "            for response in tool_responses:\n",
    "                content = response.content\n",
    "                if content and isinstance(content, list):\n",
    "                    # Content is a list of TextContentItem objects\n",
    "                    for item in content:\n",
    "                        # Check if item has text attribute and is a TextContentItem\n",
    "                        if (\n",
    "                            hasattr(item, \"text\")\n",
    "                            and hasattr(item, \"type\")\n",
    "                            and item.type == \"text\"\n",
    "                        ):\n",
    "                            text = item.text\n",
    "                            # Look for \"Result X\" patterns and extract content\n",
    "                            if (\n",
    "                                text\n",
    "                                and text.startswith(\"Result \")\n",
    "                                and \"Content:\" in text\n",
    "                            ):\n",
    "                                # Extract the content part after \"Content:\"\n",
    "                                content_match = re.search(\n",
    "                                    r\"Content:\\s*(.*?)(?=\\nMetadata:|$)\",\n",
    "                                    text,\n",
    "                                    re.DOTALL,\n",
    "                                )\n",
    "                                if content_match:\n",
    "                                    content_text = content_match.group(1).strip()\n",
    "                                    retrieved_context.append(content_text)\n",
    "\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List RAG key market use cases</td>\n",
       "      <td>[Market Use Cases Key\\nRAG is being adopted ac...</td>\n",
       "      <td>Based on the search results, RAG key market us...</td>\n",
       "      <td>\\n1. Knowledge Question Answering: Providing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Describe the sequence of steps of the Ingestio...</td>\n",
       "      <td>[Market Use Cases Key\\nRAG is being adopted ac...</td>\n",
       "      <td>Based on the search results, the sequence of s...</td>\n",
       "      <td>\\nIngestion Flow:\\n1. Document Upload (via UI/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                      List RAG key market use cases   \n",
       "1  Describe the sequence of steps of the Ingestio...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Market Use Cases Key\\nRAG is being adopted ac...   \n",
       "1  [Market Use Cases Key\\nRAG is being adopted ac...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the search results, RAG key market us...   \n",
       "1  Based on the search results, the sequence of s...   \n",
       "\n",
       "                                           reference  \n",
       "0  \\n1. Knowledge Question Answering: Providing a...  \n",
       "1  \\nIngestion Flow:\\n1. Document Upload (via UI/...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = ['''\n",
    "1. Knowledge Question Answering: Providing accurate answers in customer service, product manuals, or FAQs.\n",
    "2. Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "3. Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "4. Customer Service: Improving support accuracy with access to current product information.\n",
    "5. Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "6. Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "7. Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "8. General Task Assistance: Aiding users in various tasks requiring information access and decision-making.''', \n",
    "'''\n",
    "Ingestion Flow:\n",
    "1. Document Upload (via UI/API)\n",
    "2. Docling (Chunking + Metadata)\n",
    "3. Embedding (Granite Embedding via vLLM)\n",
    "4. Milvus (Vector DB)\n",
    "'''\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, turn in enumerate(session_response.turns):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": turn.input_messages[0].content,\n",
    "            \"response\": turn.output_message.content,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(turn),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create .env.dev file and paste there your API Key from Groq Cloud\n",
    "1. Copy run this command: vi .env.dev\n",
    "2. GROQ_API_KEY=\"YOUR_GROQ_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env variable\n",
    "load_dotenv(dotenv_path=\".env.dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ibm-granite/granite-embedding-125m-english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved contexts for the first prompt: ['Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .', 'Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel', 'Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel']\n",
      "\n",
      "Retrieved contexts for the second prompt: ['Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .', 'Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\n- Knowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\n- Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\n- Recommendation Systems: Enhancing recommendations by providing relevant context.\\n- Customer Service: Improving support accuracy with access to current product information.\\n- Personal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\n- Multi-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\n- Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\\n- General Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .', 'Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel', 'Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness, \n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    ResponseRelevancy,\n",
    ") \n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    # model=\"llama3-8b-8192\",\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "\n",
    "reference_list_for_first_prompt = [line.strip() for line in reference_for_first_prompt.strip().split('\\n')]\n",
    "reference_list_for_second_prompt = [line.strip() for line in reference_for_second_prompt.strip().split('\\n')]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\")\n",
    "print(f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'List RAG key market use cases': 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 17.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'Describe the sequence of steps of the Ingestion Flow': 0.9375\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'List RAG key market use cases': 0.9749768611161626\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "response_relevancy_score_for_first_prompt = await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 11.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 11.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 11.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'Describe the sequence of steps of the Ingestion Flow': 0.9328236620990006\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "response_relevancy_score_for_second_prompt = await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
