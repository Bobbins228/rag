# PIPELINE DEFINITION
# Name: docling-convert-pipeline
# Description: Converts spreadsheets (csv and excel) to text using Docling and generates embeddings
# Inputs:
#    base_url: str [Default: 'https://raw.githubusercontent.com/opendatahub-io/rag/main/demos/testing-data/spreadsheets']
#    embed_model_id: str [Default: 'ibm-granite/granite-embedding-125m-english']
#    max_tokens: int [Default: 512.0]
#    num_workers: int [Default: 1.0]
#    service_url: str [Default: 'http://lsd-llama-milvus-service:8321']
#    spreadsheet_filenames: str [Default: 'people.xlsx, sample_sales_data.xlsm, test_customers.csv']
#    use_gpu: bool [Default: True]
#    vector_db_id: str [Default: 'csv-vector-db']
components:
  comp-condition-3:
    dag:
      tasks:
        docling-convert-and-ingest-spreadsheets:
          cachingOptions: {}
          componentRef:
            name: comp-docling-convert-and-ingest-spreadsheets
          inputs:
            artifacts:
              input_path:
                componentInputArtifact: pipelinechannel--import-spreadsheet-files-output_path
            parameters:
              embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              service_url:
                componentInputParameter: pipelinechannel--service_url
              spreadsheet_split:
                componentInputParameter: pipelinechannel--create-spreadsheet-splits-Output-loop-item
              vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: docling-convert-and-ingest-spreadsheets
    inputDefinitions:
      artifacts:
        pipelinechannel--import-spreadsheet-files-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-spreadsheet-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-condition-4:
    dag:
      tasks:
        docling-convert-and-ingest-spreadsheets-2:
          cachingOptions: {}
          componentRef:
            name: comp-docling-convert-and-ingest-spreadsheets-2
          inputs:
            artifacts:
              input_path:
                componentInputArtifact: pipelinechannel--import-spreadsheet-files-output_path
            parameters:
              embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              service_url:
                componentInputParameter: pipelinechannel--service_url
              spreadsheet_split:
                componentInputParameter: pipelinechannel--create-spreadsheet-splits-Output-loop-item
              vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: docling-convert-and-ingest-spreadsheets-2
    inputDefinitions:
      artifacts:
        pipelinechannel--import-spreadsheet-files-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-spreadsheet-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-condition-branches-2:
    dag:
      tasks:
        condition-3:
          componentRef:
            name: comp-condition-3
          inputs:
            artifacts:
              pipelinechannel--import-spreadsheet-files-output_path:
                componentInputArtifact: pipelinechannel--import-spreadsheet-files-output_path
            parameters:
              pipelinechannel--create-spreadsheet-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-spreadsheet-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-3
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--use_gpu'] == true
        condition-4:
          componentRef:
            name: comp-condition-4
          inputs:
            artifacts:
              pipelinechannel--import-spreadsheet-files-output_path:
                componentInputArtifact: pipelinechannel--import-spreadsheet-files-output_path
            parameters:
              pipelinechannel--create-spreadsheet-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-spreadsheet-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-4
          triggerPolicy:
            condition: '!(inputs.parameter_values[''pipelinechannel--use_gpu''] ==
              true)'
    inputDefinitions:
      artifacts:
        pipelinechannel--import-spreadsheet-files-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-spreadsheet-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-create-spreadsheet-splits:
    executorLabel: exec-create-spreadsheet-splits
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        num_splits:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-docling-convert-and-ingest-spreadsheets:
    executorLabel: exec-docling-convert-and-ingest-spreadsheets
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        embed_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        service_url:
          parameterType: STRING
        spreadsheet_split:
          parameterType: LIST
        vector_db_id:
          parameterType: STRING
  comp-docling-convert-and-ingest-spreadsheets-2:
    executorLabel: exec-docling-convert-and-ingest-spreadsheets-2
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        embed_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        service_url:
          parameterType: STRING
        spreadsheet_split:
          parameterType: LIST
        vector_db_id:
          parameterType: STRING
  comp-for-loop-1:
    dag:
      tasks:
        condition-branches-2:
          componentRef:
            name: comp-condition-branches-2
          inputs:
            artifacts:
              pipelinechannel--import-spreadsheet-files-output_path:
                componentInputArtifact: pipelinechannel--import-spreadsheet-files-output_path
            parameters:
              pipelinechannel--create-spreadsheet-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-spreadsheet-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-branches-2
    inputDefinitions:
      artifacts:
        pipelinechannel--import-spreadsheet-files-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-spreadsheet-splits-Output:
          parameterType: LIST
        pipelinechannel--create-spreadsheet-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-import-spreadsheet-files:
    executorLabel: exec-import-spreadsheet-files
    inputDefinitions:
      parameters:
        base_url:
          parameterType: STRING
        spreadsheet_filenames:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-register-vector-db:
    executorLabel: exec-register-vector-db
    inputDefinitions:
      parameters:
        embed_model_id:
          parameterType: STRING
        service_url:
          parameterType: STRING
        vector_db_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-create-spreadsheet-splits:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - create_spreadsheet_splits
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef create_spreadsheet_splits(\n    input_path: dsl.InputPath(\"\
          input-spreadsheets\"),\n    num_splits: int,\n) -> List[List[str]]:\n  \
          \  import pathlib\n\n    # Split our entire directory of spreadsheet files\
          \ into n batches, where n == num_splits\n    # Support common formats\n\
          \    spreadsheet_extensions = [\"*.csv\", \"*.xlsx\", \"*.xls\", \"*.xlsm\"\
          ]\n    all_spreadsheets = []\n\n    input_dir = pathlib.Path(input_path)\n\
          \    for ext in spreadsheet_extensions:\n        all_spreadsheets.extend([path.name\
          \ for path in input_dir.glob(ext)])\n\n    splits = [\n        batch\n \
          \       for batch in (all_spreadsheets[i::num_splits] for i in range(num_splits))\n\
          \        if batch\n    ]\n    return splits or [[]]\n\n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
    exec-docling-convert-and-ingest-spreadsheets:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - docling_convert_and_ingest_spreadsheets
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling' 'docling-core'\
          \ 'transformers' 'sentence-transformers' 'llama-stack' 'llama-stack-client'\
          \ 'pymilvus' 'fire' 'pandas>=2.3.0' 'openpyxl>=3.1.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef docling_convert_and_ingest_spreadsheets(\n    input_path: dsl.InputPath(\"\
          input-spreadsheets\"),\n    spreadsheet_split: List[str],\n    embed_model_id:\
          \ str,\n    max_tokens: int,\n    service_url: str,\n    vector_db_id: str,\n\
          ):\n    import pathlib\n    import pandas as pd\n    from docling.datamodel.base_models\
          \ import ConversionStatus\n    from docling.datamodel.document import ConversionResult\n\
          \    from docling.document_converter import DocumentConverter\n    from\
          \ docling_core.types.doc.document import DoclingDocument\n    import shutil\n\
          \    import tempfile\n    import uuid\n\n    from transformers import AutoTokenizer\n\
          \    from sentence_transformers import SentenceTransformer\n    from docling_core.transforms.chunker.hybrid_chunker\
          \ import HybridChunker\n    import logging\n    from llama_stack_client\
          \ import LlamaStackClient\n    import json\n\n    _log = logging.getLogger(__name__)\n\
          \n    # Create a truly local temp directory for processing\n    local_processing_dir\
          \ = pathlib.Path(tempfile.mkdtemp(prefix=\"docling-local-\"))\n    _log.info(f\"\
          Local processing directory: {local_processing_dir}\")\n\n    def convert_excel_to_csv(\n\
          \        input_spreadsheet_files: List[pathlib.Path], output_path: pathlib.Path\n\
          \    ) -> List[pathlib.Path]:\n        processed_csv_files = []\n\n    \
          \    for file_path in input_spreadsheet_files:\n            if not file_path.exists():\n\
          \                _log.info(f\"Skipping missing file: {file_path}\")\n  \
          \              continue\n\n            if file_path.suffix.lower() == \"\
          .csv\":\n                new_path = output_path / file_path.name\n     \
          \           try:\n                    # First, try to read it as a normal\
          \ CSV. 'infer' handles uncompressed files.\n                    df = pd.read_csv(file_path,\
          \ compression=\"infer\", engine=\"python\")\n                    _log.info(f\"\
          Read {file_path.name} as a standard CSV.\")\n\n                except (UnicodeDecodeError,\
          \ EOFError):\n                    # If it fails with a decoding error, it's\
          \ likely a misnamed compressed file.\n                    _log.warning(\n\
          \                        f\"Standard read failed for {file_path.name}. Attempting\
          \ gzip decompression.\"\n                    )\n                    try:\n\
          \                        # Second, try reading it again, but force gzip\
          \ decompression.\n                        df = pd.read_csv(file_path, compression=\"\
          gzip\", engine=\"python\")\n                        _log.info(\n       \
          \                     f\"Successfully read {file_path.name} with forced\
          \ gzip.\"\n                        )\n                    except Exception\
          \ as e:\n                        _log.error(\n                         \
          \   f\"Could not read {file_path.name} with any method. Error: {e}\"\n \
          \                       )\n                        continue\n\n        \
          \        df.to_csv(new_path, index=False)\n                processed_csv_files.append(new_path)\n\
          \n            # Check if the file is an Excel format\n            elif file_path.suffix.lower()\
          \ in [\".xlsx\", \".xls\", \".xlsm\"]:\n                _log.info(f\"Converting\
          \ {file_path.name} to CSV format...\")\n\n                try:\n       \
          \             # Use pandas to read all sheets from the Excel file\n    \
          \                excel_sheets = pd.read_excel(file_path, sheet_name=None)\n\
          \n                    for sheet_name, df in excel_sheets.items():\n    \
          \                    new_csv_filename = f\"{file_path.stem}_{sheet_name}.csv\"\
          \n                        new_csv_path = output_path / new_csv_filename\n\
          \                        df.to_csv(new_csv_path, index=False, header=True)\n\
          \                        processed_csv_files.append(new_csv_path)\n    \
          \                    _log.info(\n                            f\"Successfully\
          \ converted sheet '{sheet_name}' to '{new_csv_path.name}'\"\n          \
          \              )\n\n                except Exception as e:\n           \
          \         _log.error(f\"Excel conversion failed for {file_path.name}: {e}\"\
          )\n                    continue\n            else:\n                _log.info(f\"\
          Skipping unsupported file type: {file_path.name}\")\n\n        return processed_csv_files\n\
          \n    # ---- Embedding Helper functions ----\n    def setup_chunker_and_embedder(\n\
          \        embed_model_id: str, max_tokens: int\n    ) -> Tuple[SentenceTransformer,\
          \ HybridChunker]:\n        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)\n\
          \        embedding_model = SentenceTransformer(embed_model_id)\n       \
          \ chunker = HybridChunker(\n            tokenizer=tokenizer, max_tokens=max_tokens,\
          \ merge_peers=True\n        )\n\n        return embedding_model, chunker\n\
          \n    def embed_text(text: str, embedding_model: SentenceTransformer) ->\
          \ list[float]:\n        return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]\n\
          \n    def create_chunks_with_embeddings(\n        converted_data: DoclingDocument,\n\
          \        embedding_model: SentenceTransformer,\n        chunker: HybridChunker,\n\
          \        file_name: str,\n    ) -> List[Dict]:\n\n        _log.info(f\"\
          Docling Converted data: {converted_data}\")\n\n        chunks_with_embeddings\
          \ = []\n        for chunk in chunker.chunk(dl_doc=converted_data):\n   \
          \         _log.info(f\"Chunk: {chunk}\")\n\n            raw_chunk = chunker.contextualize(chunk)\n\
          \            _log.info(f\"Raw chunk: {raw_chunk}\")\n\n            embedding\
          \ = embed_text(raw_chunk, embedding_model)\n            _log.info(f\"Embedding:\
          \ {embedding}\")\n\n            chunk_id = str(uuid.uuid4())\n         \
          \   content_token_count = chunker.tokenizer.count_tokens(raw_chunk)\n\n\
          \            # Prepare metadata object\n            metadata_obj = {\n \
          \               \"file_name\": file_name,\n                \"document_id\"\
          : chunk_id,\n                \"token_count\": content_token_count,\n   \
          \         }\n\n            metadata_str = json.dumps(metadata_obj)\n   \
          \         metadata_token_count = chunker.tokenizer.count_tokens(metadata_str)\n\
          \            metadata_obj[\"metadata_token_count\"] = metadata_token_count\n\
          \n            # Create a new chunk with embedding\n            new_chunk_with_embedding\
          \ = {\n                \"content\": raw_chunk,\n                \"mime_type\"\
          : \"text/markdown\",\n                \"embedding\": embedding,\n      \
          \          \"metadata\": metadata_obj,\n            }\n\n            _log.info(f\"\
          New embedding: {new_chunk_with_embedding}\")\n\n            chunks_with_embeddings.append(new_chunk_with_embedding)\n\
          \n        _log.info(f\"Chunks with embeddings: {chunks_with_embeddings}\"\
          )\n\n        return chunks_with_embeddings\n\n    def insert_chunks_with_embeddings_to_vector_db(\n\
          \        chunks_with_embeddings: List[Dict],\n        vector_db_id: str,\n\
          \        client: LlamaStackClient,\n    ) -> None:\n        _log.info(\n\
          \            f\"Inserting chunks with embeddings to vector database: {chunks_with_embeddings}\"\
          \n        )\n\n        if chunks_with_embeddings:\n            try:\n  \
          \              client.vector_io.insert(\n                    vector_db_id=vector_db_id,\
          \ chunks=chunks_with_embeddings\n                )\n            except Exception\
          \ as e:\n                _log.error(f\"Failed to insert embeddings into\
          \ vector database: {e}\")\n\n    def process_conversion_results(\n     \
          \   conv_results: Iterator[ConversionResult], client: LlamaStackClient\n\
          \    ) -> None:\n        processed_docs = 0\n        embedding_model, chunker\
          \ = setup_chunker_and_embedder(\n            embed_model_id, max_tokens\n\
          \        )\n        for conv_res in conv_results:\n            if conv_res.status\
          \ != ConversionStatus.SUCCESS:\n                _log.warning(\n        \
          \            f\"Conversion failed for {conv_res.input.file.stem}: {conv_res.status}\"\
          \n                )\n                continue\n\n            processed_docs\
          \ += 1\n            file_name = conv_res.input.file.stem\n            document\
          \ = conv_res.document\n\n            if document is None:\n            \
          \    _log.warning(f\"Document conversion failed for {file_name}\")\n   \
          \             continue\n\n            chunks_with_embeddings = create_chunks_with_embeddings(\n\
          \                document, embedding_model, chunker, file_name\n       \
          \     )\n\n            insert_chunks_with_embeddings_to_vector_db(\n   \
          \             chunks_with_embeddings, vector_db_id, client\n           \
          \ )\n\n        _log.info(f\"Processed {processed_docs} documents successfully.\"\
          )\n\n    input_path = pathlib.Path(input_path)\n\n    # Copy input files\
          \ to the local processing dir\n    input_spreadsheets_files = [input_path\
          \ / name for name in spreadsheet_split]\n    csv_files = convert_excel_to_csv(input_spreadsheets_files,\
          \ local_processing_dir)\n\n    _log.info(f\"CSV files for Docling: {csv_files}\"\
          )\n\n    docling_csv_converter = DocumentConverter()\n\n    # Convert all\
          \ spreadsheet files to text\n    conv_results = docling_csv_converter.convert_all(\n\
          \        csv_files,\n        raises_on_error=True,\n    )\n\n    client\
          \ = LlamaStackClient(base_url=service_url)\n\n    process_conversion_results(conv_results,\
          \ client)\n\n    # Clean up the local processing directory\n    shutil.rmtree(local_processing_dir)\n\
          \    _log.info(f\"Cleaned up local processing directory: {local_processing_dir}\"\
          )\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py311-ubi9@sha256:4706be608af3f33c88700ef6ef6a99e716fc95fc7d2e879502e81c0022fd840e
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
            resourceType: nvidia.com/gpu
            type: nvidia.com/gpu
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-docling-convert-and-ingest-spreadsheets-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - docling_convert_and_ingest_spreadsheets
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling' 'docling-core'\
          \ 'transformers' 'sentence-transformers' 'llama-stack' 'llama-stack-client'\
          \ 'pymilvus' 'fire' 'pandas>=2.3.0' 'openpyxl>=3.1.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef docling_convert_and_ingest_spreadsheets(\n    input_path: dsl.InputPath(\"\
          input-spreadsheets\"),\n    spreadsheet_split: List[str],\n    embed_model_id:\
          \ str,\n    max_tokens: int,\n    service_url: str,\n    vector_db_id: str,\n\
          ):\n    import pathlib\n    import pandas as pd\n    from docling.datamodel.base_models\
          \ import ConversionStatus\n    from docling.datamodel.document import ConversionResult\n\
          \    from docling.document_converter import DocumentConverter\n    from\
          \ docling_core.types.doc.document import DoclingDocument\n    import shutil\n\
          \    import tempfile\n    import uuid\n\n    from transformers import AutoTokenizer\n\
          \    from sentence_transformers import SentenceTransformer\n    from docling_core.transforms.chunker.hybrid_chunker\
          \ import HybridChunker\n    import logging\n    from llama_stack_client\
          \ import LlamaStackClient\n    import json\n\n    _log = logging.getLogger(__name__)\n\
          \n    # Create a truly local temp directory for processing\n    local_processing_dir\
          \ = pathlib.Path(tempfile.mkdtemp(prefix=\"docling-local-\"))\n    _log.info(f\"\
          Local processing directory: {local_processing_dir}\")\n\n    def convert_excel_to_csv(\n\
          \        input_spreadsheet_files: List[pathlib.Path], output_path: pathlib.Path\n\
          \    ) -> List[pathlib.Path]:\n        processed_csv_files = []\n\n    \
          \    for file_path in input_spreadsheet_files:\n            if not file_path.exists():\n\
          \                _log.info(f\"Skipping missing file: {file_path}\")\n  \
          \              continue\n\n            if file_path.suffix.lower() == \"\
          .csv\":\n                new_path = output_path / file_path.name\n     \
          \           try:\n                    # First, try to read it as a normal\
          \ CSV. 'infer' handles uncompressed files.\n                    df = pd.read_csv(file_path,\
          \ compression=\"infer\", engine=\"python\")\n                    _log.info(f\"\
          Read {file_path.name} as a standard CSV.\")\n\n                except (UnicodeDecodeError,\
          \ EOFError):\n                    # If it fails with a decoding error, it's\
          \ likely a misnamed compressed file.\n                    _log.warning(\n\
          \                        f\"Standard read failed for {file_path.name}. Attempting\
          \ gzip decompression.\"\n                    )\n                    try:\n\
          \                        # Second, try reading it again, but force gzip\
          \ decompression.\n                        df = pd.read_csv(file_path, compression=\"\
          gzip\", engine=\"python\")\n                        _log.info(\n       \
          \                     f\"Successfully read {file_path.name} with forced\
          \ gzip.\"\n                        )\n                    except Exception\
          \ as e:\n                        _log.error(\n                         \
          \   f\"Could not read {file_path.name} with any method. Error: {e}\"\n \
          \                       )\n                        continue\n\n        \
          \        df.to_csv(new_path, index=False)\n                processed_csv_files.append(new_path)\n\
          \n            # Check if the file is an Excel format\n            elif file_path.suffix.lower()\
          \ in [\".xlsx\", \".xls\", \".xlsm\"]:\n                _log.info(f\"Converting\
          \ {file_path.name} to CSV format...\")\n\n                try:\n       \
          \             # Use pandas to read all sheets from the Excel file\n    \
          \                excel_sheets = pd.read_excel(file_path, sheet_name=None)\n\
          \n                    for sheet_name, df in excel_sheets.items():\n    \
          \                    new_csv_filename = f\"{file_path.stem}_{sheet_name}.csv\"\
          \n                        new_csv_path = output_path / new_csv_filename\n\
          \                        df.to_csv(new_csv_path, index=False, header=True)\n\
          \                        processed_csv_files.append(new_csv_path)\n    \
          \                    _log.info(\n                            f\"Successfully\
          \ converted sheet '{sheet_name}' to '{new_csv_path.name}'\"\n          \
          \              )\n\n                except Exception as e:\n           \
          \         _log.error(f\"Excel conversion failed for {file_path.name}: {e}\"\
          )\n                    continue\n            else:\n                _log.info(f\"\
          Skipping unsupported file type: {file_path.name}\")\n\n        return processed_csv_files\n\
          \n    # ---- Embedding Helper functions ----\n    def setup_chunker_and_embedder(\n\
          \        embed_model_id: str, max_tokens: int\n    ) -> Tuple[SentenceTransformer,\
          \ HybridChunker]:\n        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)\n\
          \        embedding_model = SentenceTransformer(embed_model_id)\n       \
          \ chunker = HybridChunker(\n            tokenizer=tokenizer, max_tokens=max_tokens,\
          \ merge_peers=True\n        )\n\n        return embedding_model, chunker\n\
          \n    def embed_text(text: str, embedding_model: SentenceTransformer) ->\
          \ list[float]:\n        return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]\n\
          \n    def create_chunks_with_embeddings(\n        converted_data: DoclingDocument,\n\
          \        embedding_model: SentenceTransformer,\n        chunker: HybridChunker,\n\
          \        file_name: str,\n    ) -> List[Dict]:\n\n        _log.info(f\"\
          Docling Converted data: {converted_data}\")\n\n        chunks_with_embeddings\
          \ = []\n        for chunk in chunker.chunk(dl_doc=converted_data):\n   \
          \         _log.info(f\"Chunk: {chunk}\")\n\n            raw_chunk = chunker.contextualize(chunk)\n\
          \            _log.info(f\"Raw chunk: {raw_chunk}\")\n\n            embedding\
          \ = embed_text(raw_chunk, embedding_model)\n            _log.info(f\"Embedding:\
          \ {embedding}\")\n\n            chunk_id = str(uuid.uuid4())\n         \
          \   content_token_count = chunker.tokenizer.count_tokens(raw_chunk)\n\n\
          \            # Prepare metadata object\n            metadata_obj = {\n \
          \               \"file_name\": file_name,\n                \"document_id\"\
          : chunk_id,\n                \"token_count\": content_token_count,\n   \
          \         }\n\n            metadata_str = json.dumps(metadata_obj)\n   \
          \         metadata_token_count = chunker.tokenizer.count_tokens(metadata_str)\n\
          \            metadata_obj[\"metadata_token_count\"] = metadata_token_count\n\
          \n            # Create a new chunk with embedding\n            new_chunk_with_embedding\
          \ = {\n                \"content\": raw_chunk,\n                \"mime_type\"\
          : \"text/markdown\",\n                \"embedding\": embedding,\n      \
          \          \"metadata\": metadata_obj,\n            }\n\n            _log.info(f\"\
          New embedding: {new_chunk_with_embedding}\")\n\n            chunks_with_embeddings.append(new_chunk_with_embedding)\n\
          \n        _log.info(f\"Chunks with embeddings: {chunks_with_embeddings}\"\
          )\n\n        return chunks_with_embeddings\n\n    def insert_chunks_with_embeddings_to_vector_db(\n\
          \        chunks_with_embeddings: List[Dict],\n        vector_db_id: str,\n\
          \        client: LlamaStackClient,\n    ) -> None:\n        _log.info(\n\
          \            f\"Inserting chunks with embeddings to vector database: {chunks_with_embeddings}\"\
          \n        )\n\n        if chunks_with_embeddings:\n            try:\n  \
          \              client.vector_io.insert(\n                    vector_db_id=vector_db_id,\
          \ chunks=chunks_with_embeddings\n                )\n            except Exception\
          \ as e:\n                _log.error(f\"Failed to insert embeddings into\
          \ vector database: {e}\")\n\n    def process_conversion_results(\n     \
          \   conv_results: Iterator[ConversionResult], client: LlamaStackClient\n\
          \    ) -> None:\n        processed_docs = 0\n        embedding_model, chunker\
          \ = setup_chunker_and_embedder(\n            embed_model_id, max_tokens\n\
          \        )\n        for conv_res in conv_results:\n            if conv_res.status\
          \ != ConversionStatus.SUCCESS:\n                _log.warning(\n        \
          \            f\"Conversion failed for {conv_res.input.file.stem}: {conv_res.status}\"\
          \n                )\n                continue\n\n            processed_docs\
          \ += 1\n            file_name = conv_res.input.file.stem\n            document\
          \ = conv_res.document\n\n            if document is None:\n            \
          \    _log.warning(f\"Document conversion failed for {file_name}\")\n   \
          \             continue\n\n            chunks_with_embeddings = create_chunks_with_embeddings(\n\
          \                document, embedding_model, chunker, file_name\n       \
          \     )\n\n            insert_chunks_with_embeddings_to_vector_db(\n   \
          \             chunks_with_embeddings, vector_db_id, client\n           \
          \ )\n\n        _log.info(f\"Processed {processed_docs} documents successfully.\"\
          )\n\n    input_path = pathlib.Path(input_path)\n\n    # Copy input files\
          \ to the local processing dir\n    input_spreadsheets_files = [input_path\
          \ / name for name in spreadsheet_split]\n    csv_files = convert_excel_to_csv(input_spreadsheets_files,\
          \ local_processing_dir)\n\n    _log.info(f\"CSV files for Docling: {csv_files}\"\
          )\n\n    docling_csv_converter = DocumentConverter()\n\n    # Convert all\
          \ spreadsheet files to text\n    conv_results = docling_csv_converter.convert_all(\n\
          \        csv_files,\n        raises_on_error=True,\n    )\n\n    client\
          \ = LlamaStackClient(base_url=service_url)\n\n    process_conversion_results(conv_results,\
          \ client)\n\n    # Clean up the local processing directory\n    shutil.rmtree(local_processing_dir)\n\
          \    _log.info(f\"Cleaned up local processing directory: {local_processing_dir}\"\
          )\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py311-ubi9@sha256:4706be608af3f33c88700ef6ef6a99e716fc95fc7d2e879502e81c0022fd840e
        resources:
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-import-spreadsheet-files:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - import_spreadsheet_files
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'requests' &&\
          \ \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef import_spreadsheet_files(\n    base_url: str,\n    spreadsheet_filenames:\
          \ str,\n    output_path: dsl.OutputPath(\"input-spreadsheets\"),\n):\n \
          \   import os\n    import requests\n    import shutil\n\n    os.makedirs(output_path,\
          \ exist_ok=True)\n    filenames = [f.strip() for f in spreadsheet_filenames.split(\"\
          ,\") if f.strip()]\n\n    for filename in filenames:\n        url = f\"\
          {base_url.rstrip('/')}/{filename}\"\n        file_path = os.path.join(output_path,\
          \ filename)\n\n        try:\n            with requests.get(url, stream=True,\
          \ timeout=30) as response:\n                response.raise_for_status()\n\
          \                with open(file_path, \"wb\") as f:\n                  \
          \  shutil.copyfileobj(response.raw, f)\n            print(f\"Downloaded\
          \ {filename}\")\n        except requests.exceptions.RequestException as\
          \ e:\n            print(f\"Failed to download {filename}: {e}, skipping.\"\
          )\n\n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
    exec-register-vector-db:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - register_vector_db
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'llama-stack-client'\
          \ 'fire' 'requests' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef register_vector_db(\n    service_url: str,\n    vector_db_id:\
          \ str,\n    embed_model_id: str,\n):\n    from llama_stack_client import\
          \ LlamaStackClient\n\n    client = LlamaStackClient(base_url=service_url)\n\
          \n    models = client.models.list()\n    matching_model = next(\n      \
          \  (m for m in models if m.provider_resource_id == embed_model_id), None\n\
          \    )\n\n    if not matching_model:\n        raise ValueError(\n      \
          \      f\"Model with ID '{embed_model_id}' not found on LlamaStack server.\"\
          \n        )\n\n    if matching_model.model_type != \"embedding\":\n    \
          \    raise ValueError(f\"Model '{embed_model_id}' is not an embedding model\"\
          )\n\n    embedding_dimension = matching_model.metadata[\"embedding_dimension\"\
          ]\n\n    _ = client.vector_dbs.register(\n        vector_db_id=vector_db_id,\n\
          \        embedding_model=matching_model.identifier,\n        embedding_dimension=embedding_dimension,\n\
          \        provider_id=\"milvus\",\n    )\n    print(\n        f\"Registered\
          \ vector DB '{vector_db_id}' with embedding model '{embed_model_id}'.\"\n\
          \    )\n\n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
pipelineInfo:
  description: Converts spreadsheets (csv and excel) to text using Docling and generates
    embeddings
  name: docling-convert-pipeline
root:
  dag:
    tasks:
      create-spreadsheet-splits:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-create-spreadsheet-splits
        dependentTasks:
        - import-spreadsheet-files
        inputs:
          artifacts:
            input_path:
              taskOutputArtifact:
                outputArtifactKey: output_path
                producerTask: import-spreadsheet-files
          parameters:
            num_splits:
              componentInputParameter: num_workers
        taskInfo:
          name: create-spreadsheet-splits
      for-loop-1:
        componentRef:
          name: comp-for-loop-1
        dependentTasks:
        - create-spreadsheet-splits
        - import-spreadsheet-files
        inputs:
          artifacts:
            pipelinechannel--import-spreadsheet-files-output_path:
              taskOutputArtifact:
                outputArtifactKey: output_path
                producerTask: import-spreadsheet-files
          parameters:
            pipelinechannel--create-spreadsheet-splits-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: create-spreadsheet-splits
            pipelinechannel--embed_model_id:
              componentInputParameter: embed_model_id
            pipelinechannel--max_tokens:
              componentInputParameter: max_tokens
            pipelinechannel--service_url:
              componentInputParameter: service_url
            pipelinechannel--use_gpu:
              componentInputParameter: use_gpu
            pipelinechannel--vector_db_id:
              componentInputParameter: vector_db_id
        parameterIterator:
          itemInput: pipelinechannel--create-spreadsheet-splits-Output-loop-item
          items:
            inputParameter: pipelinechannel--create-spreadsheet-splits-Output
        taskInfo:
          name: for-loop-1
      import-spreadsheet-files:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-import-spreadsheet-files
        inputs:
          parameters:
            base_url:
              componentInputParameter: base_url
            spreadsheet_filenames:
              componentInputParameter: spreadsheet_filenames
        taskInfo:
          name: import-spreadsheet-files
      register-vector-db:
        cachingOptions: {}
        componentRef:
          name: comp-register-vector-db
        inputs:
          parameters:
            embed_model_id:
              componentInputParameter: embed_model_id
            service_url:
              componentInputParameter: service_url
            vector_db_id:
              componentInputParameter: vector_db_id
        taskInfo:
          name: register-vector-db
  inputDefinitions:
    parameters:
      base_url:
        defaultValue: https://raw.githubusercontent.com/opendatahub-io/rag/main/demos/testing-data/spreadsheets
        description: Base URL to fetch spreadsheets
        isOptional: true
        parameterType: STRING
      embed_model_id:
        defaultValue: ibm-granite/granite-embedding-125m-english
        description: Model ID for embedding generation
        isOptional: true
        parameterType: STRING
      max_tokens:
        defaultValue: 512.0
        description: Maximum number of tokens per chunk
        isOptional: true
        parameterType: NUMBER_INTEGER
      num_workers:
        defaultValue: 1.0
        description: Number of docling worker pods to use
        isOptional: true
        parameterType: NUMBER_INTEGER
      service_url:
        defaultValue: http://lsd-llama-milvus-service:8321
        description: URL of the LlamaStack service
        isOptional: true
        parameterType: STRING
      spreadsheet_filenames:
        defaultValue: people.xlsx, sample_sales_data.xlsm, test_customers.csv
        description: Comma-separated list of spreadsheets filenames to download and
          convert
        isOptional: true
        parameterType: STRING
      use_gpu:
        defaultValue: true
        description: boolean to enable/disable gpu in the docling workers
        isOptional: true
        parameterType: BOOLEAN
      vector_db_id:
        defaultValue: csv-vector-db
        description: ID of the vector database to store embeddings
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-docling-convert-and-ingest-spreadsheets:
          nodeSelector:
            nodeSelectorJson:
              runtimeValue:
                constant: {}
          tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
