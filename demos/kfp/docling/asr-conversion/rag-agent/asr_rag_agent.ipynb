{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models information: [Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "\n",
      "Identifier for Inference model in usage: vllm\n",
      "\n",
      "=== Available Vector Databases ===\n",
      "- ID: asr-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: granite-embedding-125m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next((model.identifier for model in models if model.model_type == 'llm'), None)\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create RAG Agent and prompt the LLM\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-82.74]  cognitive\\n[time: 82.74-96.7]  regulatory\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'd4b12c3f-403a-43a2-8c40-51a4c8b258f6'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '28b4f515-8a30-47c7-aff4-d0c5fee38cce'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-86.2]  Marjorie\\n[time: 86.2-93.53999999999999]  Notre\\n[time: 93.53999999999999-95.62]  Hay\\n[time: 95.62-96.9]  Cha\\n[time: 96.9-100.1]  Cha\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'c66b7246-e65c-4950-95fc-56d5f99d85b6'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.67999999999999]  Legal applications.\\n[time: 80.67999999999999-81.66]  At this time,\\n[time: 81.66-90.36]  of course,\\n[time: 90.96-95.98]  deep search drives through minder locations,\\n[time: 96.7-98.82]  of course the source app leads to�니다.\\n[time: 99.36-100.8] ệt However,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'a937ab44-6d3a-4fb6-abd7-4e696b799ee6'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 109.0-118.0]  For developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\n[time: 118.0-122.0]  Data sovereignty and privacy\\n[time: 122.0-131.0]  Using confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\n[time: 131.0-143.0]  RAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\n[time: 143.0-151.0]  RAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\n[time: 151.0-158.0]  That is, certain users can access certain information based on their security clearance levels.\\n[time: 158.0-159.0]  From friendly to aggregate to a type of research, rituals ideas?\\n[time: 159.0-163.0]  So, once a application arrives there is RAC.\\n[time: 163.0-168.0]  Overups are really accurate, it takes time to find things threshold.\\n[time: 168.0-170.66]  That is a test目 of naming that the data feature that those partners in the center stands will come out.\\n[time: 171.0-173.0]  Alright, guys, so what do you make throughout this area?\\n[time: 174.0-176.0]  Let us hear that, for sure, what are the dec Mariotaetri....\\n[time: 177.0-178.5]  ...through Abgah the Aliya ?\\n[time: 179.0-181.0]  All right.\\n[time: 181.0-182.5]  You can find the STOP.\\n[time: 183.0-185.5]  I assume that, perfectly happy that we hold back over here never wasting all the data.\\nMetadata: {'file_name': 'tmp2kzxuq3p_RAG_benefits', 'document_id': '1fb2e9a4-d349-4ca5-9756-e13e2798602e'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mactive\u001b[0m\u001b[33m Architecture\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m using\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m fax\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Enh\u001b[0m\u001b[33mancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m11\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-h\u001b[0m\u001b[33mub\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m12\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m13\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m14\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m15\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m task\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m16\u001b[0m\u001b[33m.\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m making\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m17\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m and\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m more\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Name Red Hat RAG target audience and customers\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Red Hat RAG target audience and customers'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a ChatGPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as ChatGPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'e4308f53-6b08-449c-b1d0-9a4df25b417c'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'd7680b99-7fa9-471f-8b9b-17596491d077'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'efdce117-03a3-4d16-aa57-2177eb38d055'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '7764e39c-c6a5-4b89-9242-e72269f9bed4'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '28b4f515-8a30-47c7-aff4-d0c5fee38cce'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Red Hat RAG target audience and customers\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m and\u001b[0m\u001b[33m customers\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Builders\u001b[0m\u001b[33m:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m includes\u001b[0m\u001b[33m high\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m who\u001b[0m\u001b[33m prefer\u001b[0m\u001b[33m SDK\u001b[0m\u001b[33ms\u001b[0m\u001b[33m and\u001b[0m\u001b[33m code\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m solutions\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m who\u001b[0m\u001b[33m prefer\u001b[0m\u001b[33m UI\u001b[0m\u001b[33m-driven\u001b[0m\u001b[33m workflows\u001b[0m\u001b[33m and\u001b[0m\u001b[33m visual\u001b[0m\u001b[33m tools\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m Engineers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m primitives\u001b[0m\u001b[33m to\u001b[0m\u001b[33m build\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m Dev\u001b[0m\u001b[33ms\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m primitives\u001b[0m\u001b[33m to\u001b[0m\u001b[33m build\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33mers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33m Engineers\u001b[0m\u001b[33m who\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m primitives\u001b[0m\u001b[33m to\u001b[0m\u001b[33m build\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m End\u001b[0m\u001b[33m users\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m consume\u001b[0m\u001b[33m the\u001b[0m\u001b[33m final\u001b[0m\u001b[33m product\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m are\u001b[0m\u001b[33m not\u001b[0m\u001b[33m the\u001b[0m\u001b[33m primary\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIt\u001b[0m\u001b[33m's\u001b[0m\u001b[33m worth\u001b[0m\u001b[33m noting\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m is\u001b[0m\u001b[33m not\u001b[0m\u001b[33m explicitly\u001b[0m\u001b[33m stated\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m information\u001b[0m\u001b[33m provided\u001b[0m\u001b[33m is\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m language\u001b[0m\u001b[33m used\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Name key RAG benefits\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG benefits'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '28b4f515-8a30-47c7-aff4-d0c5fee38cce'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-82.74]  cognitive\\n[time: 82.74-96.7]  regulatory\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'd4b12c3f-403a-43a2-8c40-51a4c8b258f6'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-86.2]  Marjorie\\n[time: 86.2-93.53999999999999]  Notre\\n[time: 93.53999999999999-95.62]  Hay\\n[time: 95.62-96.9]  Cha\\n[time: 96.9-100.1]  Cha\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'c66b7246-e65c-4950-95fc-56d5f99d85b6'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.67999999999999]  Legal applications.\\n[time: 80.67999999999999-81.66]  At this time,\\n[time: 81.66-90.36]  of course,\\n[time: 90.96-95.98]  deep search drives through minder locations,\\n[time: 96.7-98.82]  of course the source app leads to�니다.\\n[time: 99.36-100.8] ệt However,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'a937ab44-6d3a-4fb6-abd7-4e696b799ee6'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 109.0-118.0]  For developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\n[time: 118.0-122.0]  Data sovereignty and privacy\\n[time: 122.0-131.0]  Using confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\n[time: 131.0-143.0]  RAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\n[time: 143.0-151.0]  RAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\n[time: 151.0-158.0]  That is, certain users can access certain information based on their security clearance levels.\\n[time: 158.0-165.46]  fend pacifly\\n[time: 165.46-167.74]  What group deaths use is카 regime\\n[time: 167.74-172.88]  Frederic\\n[time: 172.88-174.92000000000002]  We are working on Chinese\\n[time: 174.92000000000002-176.88]  Paed tow\\n[time: 176.88-180.38]  The\\n[time: 181.0-183.06]  Author\\nMetadata: {'file_name': 'tmpo6gyeldb_RAG_benefits', 'document_id': '75eaa11f-7da4-4d81-9b24-452ecdb59473'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG benefits\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mactive\u001b[0m\u001b[33m Architecture\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m domain\u001b[0m\u001b[33m-specific\u001b[0m\u001b[33m and\u001b[0m\u001b[33m up\u001b[0m\u001b[33m-to\u001b[0m\u001b[33m-date\u001b[0m\u001b[33m information\u001b[0m\u001b[33m for\u001b[0m\u001b[33m developers\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Sol\u001b[0m\u001b[33mving\u001b[0m\u001b[33m privacy\u001b[0m\u001b[33m concerns\u001b[0m\u001b[33m by\u001b[0m\u001b[33m allowing\u001b[0m\u001b[33m sensitive\u001b[0m\u001b[33m data\u001b[0m\u001b[33m to\u001b[0m\u001b[33m remain\u001b[0m\u001b[33m on\u001b[0m\u001b[33m-pre\u001b[0m\u001b[33mmise\u001b[0m\u001b[33m while\u001b[0m\u001b[33m still\u001b[0m\u001b[33m being\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m inform\u001b[0m\u001b[33m a\u001b[0m\u001b[33m local\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m or\u001b[0m\u001b[33m a\u001b[0m\u001b[33m trusted\u001b[0m\u001b[33m external\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Restr\u001b[0m\u001b[33mict\u001b[0m\u001b[33ming\u001b[0m\u001b[33m sensitive\u001b[0m\u001b[33m information\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m to\u001b[0m\u001b[33m different\u001b[0m\u001b[33m authorization\u001b[0m\u001b[33m levels\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m users\u001b[0m\u001b[33m to\u001b[0m\u001b[33m access\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m information\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m their\u001b[0m\u001b[33m security\u001b[0m\u001b[33m clearance\u001b[0m\u001b[33m levels\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Fac\u001b[0m\u001b[33militating\u001b[0m\u001b[33m the\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m of\u001b[0m\u001b[33m hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m making\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m and\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m more\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Regular LLM output disadvantages\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Regular LLM output disadvantages'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-5.0]  RAC vs. Regular LLM Outputs\\n[time: 5.0-13.0]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.0-24.0]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.0-30.0]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.0-40.0]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 40.0-48.0]  LLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 48.0-54.0]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 54.0-63.0]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 63.0-75.0]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.0-87.0]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 87.0-89.0]  However, the purpose of the chart itself było as multi-sy jóilly.\\n[time: 89.0-90.5]  D protocol has also prepared, making solutions for innovation.\\n[time: 90.5-104.0]  The application of LLMS are dla fundamenta, with a dostate, with an ACA space, which means lack takia, бор dassel-A över transaction SAMS entsprechend\\n[time: 104.0-109.0]  The application of theower engine data institute abreast are aged Ukrain' literature.\\nMetadata: {'file_name': 'tmp4b_2xw2d_RAG_vs_Regular_LLM_Output', 'document_id': '9ae3b57f-310d-4704-ae99-060e680b4813'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 0.0-5.0]  RAC vs. Regular LLM Outputs\\n[time: 5.0-13.0]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.0-24.0]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.0-30.0]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.0-40.0]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 40.0-48.0]  LLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 48.0-54.0]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 54.0-63.0]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 63.0-75.0]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.0-87.0]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 87.0-113.76]  maybe foreign\\nMetadata: {'file_name': 'tmpjala_ypt_RAG_vs_Regular_LLM_Output', 'document_id': '06d87a5b-99e2-4220-84b7-d05562590069'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-5.0]  RAC vs. Regular LLM Outputs\\n[time: 5.0-13.0]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.0-24.0]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.0-30.0]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.0-40.0]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 40.0-48.0]  LLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 48.0-54.0]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 54.0-63.0]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 63.0-75.0]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.0-87.0]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 87.0-116.48]  Delar\\nMetadata: {'file_name': 'tmpqdg9qiis_RAG_vs_Regular_LLM_Output', 'document_id': '8674cdf4-d214-42bd-94fb-4538a8602941'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-5.0]  RAC vs. Regular LLM Outputs\\n[time: 5.0-13.0]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.0-24.0]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.0-30.0]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.0-40.0]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 40.0-48.0]  LLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 48.0-54.0]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 54.0-63.0]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 63.0-75.0]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.0-87.0]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 87.0-88.0]  LLMs can be totallyAIy.\\n[time: 88.0-90.0]  LLMs can meet exactly based on or renewable and international information produzies at times for all kinds.\\n[time: 90.0-96.86]  LLMs can beєtaє D7HC the file and drastically\\n[time: 96.86-100.0]  LLMs can inclusion and unity in convenient space and has threatened information.\\nMetadata: {'file_name': 'tmppm182gp6_RAG_vs_Regular_LLM_Output', 'document_id': 'f17dd36f-b805-46ea-a91d-e20a455ec28a'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 0.0-3.0]  The benefits of RAC\\n[time: 3.0-12.0]  The retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\n[time: 12.0-21.0]  Grounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\n[time: 21.0-22.0]  Accuracy\\n[time: 22.0-27.0]  RAC provides an LLM with sources it can cite so users can verify these claims.\\n[time: 27.0-34.0]  You can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\n[time: 34.0-44.0]  Overall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\n[time: 44.0-46.0]  Cost effectiveness\\n[time: 46.0-56.0]  Retraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\n[time: 56.0-66.0]  With RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\n[time: 66.0-70.0]  RAC can also reduce inference codes.\\n[time: 70.0-84.0]  LLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\n[time: 84.0-97.0]  Rather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\n[time: 97.0-99.0]  Developer Control\\n[time: 99.0-109.0]  Compared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nMetadata: {'file_name': 'tmp2kzxuq3p_RAG_benefits', 'document_id': '06c9dab3-f156-494c-9b43-8f003329c43e'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Regular LLM output disadvantages\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m of\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLarge\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Model\u001b[0m\u001b[33m)\u001b[0m\u001b[33m output\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m internal\u001b[0m\u001b[33m data\u001b[0m\u001b[33m sets\u001b[0m\u001b[33m from\u001b[0m\u001b[33m an\u001b[0m\u001b[33m organization\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m cut\u001b[0m\u001b[33m-off\u001b[0m\u001b[33m date\u001b[0m\u001b[33m,\u001b[0m\u001b[33m meaning\u001b[0m\u001b[33m the\u001b[0m\u001b[33m information\u001b[0m\u001b[33m they\u001b[0m\u001b[33m've\u001b[0m\u001b[33m been\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m doesn\u001b[0m\u001b[33m't\u001b[0m\u001b[33m continuously\u001b[0m\u001b[33m gather\u001b[0m\u001b[33m updates\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Resource\u001b[0m\u001b[33m material\u001b[0m\u001b[33m can\u001b[0m\u001b[33m become\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m and\u001b[0m\u001b[33m no\u001b[0m\u001b[33m longer\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m E\u001b[0m\u001b[33mager\u001b[0m\u001b[33m to\u001b[0m\u001b[33m please\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m means\u001b[0m\u001b[33m they\u001b[0m\u001b[33m sometimes\u001b[0m\u001b[33m present\u001b[0m\u001b[33m false\u001b[0m\u001b[33m or\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m also\u001b[0m\u001b[33m known\u001b[0m\u001b[33m as\u001b[0m\u001b[33m halluc\u001b[0m\u001b[33mination\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m ability\u001b[0m\u001b[33m to\u001b[0m\u001b[33m verify\u001b[0m\u001b[33m claims\u001b[0m\u001b[33m made\u001b[0m\u001b[33m by\u001b[0m\u001b[33m the\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m High\u001b[0m\u001b[33m cost\u001b[0m\u001b[33m of\u001b[0m\u001b[33m re\u001b[0m\u001b[33mtraining\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m In\u001b[0m\u001b[33mference\u001b[0m\u001b[33m codes\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m expensive\u001b[0m\u001b[33m,\u001b[0m\u001b[33m placing\u001b[0m\u001b[33m demands\u001b[0m\u001b[33m on\u001b[0m\u001b[33m hardware\u001b[0m\u001b[33m or\u001b[0m\u001b[33m increasing\u001b[0m\u001b[33m bills\u001b[0m\u001b[33m for\u001b[0m\u001b[33m external\u001b[0m\u001b[33m services\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m queries\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m large\u001b[0m\u001b[33m,\u001b[0m\u001b[33m sending\u001b[0m\u001b[33m entire\u001b[0m\u001b[33m reference\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m at\u001b[0m\u001b[33m once\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m developer\u001b[0m\u001b[33m control\u001b[0m\u001b[33m,\u001b[0m\u001b[33m making\u001b[0m\u001b[33m it\u001b[0m\u001b[33m difficult\u001b[0m\u001b[33m to\u001b[0m\u001b[33m get\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m,\u001b[0m\u001b[33m troub\u001b[0m\u001b[33mleshoot\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fix\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m limitations\u001b[0m\u001b[33m and\u001b[0m\u001b[33m challenges\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m how\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mactive\u001b[0m\u001b[33m Architecture\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m can\u001b[0m\u001b[33m address\u001b[0m\u001b[33m these\u001b[0m\u001b[33m issues\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m output\u001b[0m\u001b[33m for\u001b[0m\u001b[33m users\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> What is the economics condition at Ireland in 2025?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'economics condition in Ireland in 2025'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 215.85999999999999-216.85999999999999]  Work\\n[time: 216.85999999999999-216.94]  Work\\n[time: 216.94-217.85999999999999]  Work\\n[time: 217.85999999999999-218.2]  В\\n[time: 218.2-218.85999999999999]  Work\\n[time: 218.85999999999999-219.39999999999998]  Work\\n[time: 219.39999999999998-219.76]  Work\\n[time: 219.76-219.85999999999999]  Work\\n[time: 219.85999999999999-220.2]  It's\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '60260e32-36d6-471f-8e40-7d1d827091b2'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 100.0-105.0]  LLMs enjoy the air conditioning, which means we look at it at the current médico气ty email soűаемся.\\n[time: 105.0-107.0]  LLMs enjoy Vivian Square challenging speech which means it supports alternative before high customization.\\n[time: 107.0-110.0]  LLMs use kadę D8W and PR settings andcktTEMPY in action in elementary mode.\\n[time: 110.0-116.0]  And LLMs are new機 feels very early to a longitude,する website itself.\\nMetadata: {'file_name': 'tmppm182gp6_RAG_vs_Regular_LLM_Output', 'document_id': '03d40016-d54f-4136-b14d-2936a74c03df'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '28b4f515-8a30-47c7-aff4-d0c5fee38cce'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.67999999999999]  Legal applications.\\n[time: 80.67999999999999-81.66]  At this time,\\n[time: 81.66-90.36]  of course,\\n[time: 90.96-95.98]  deep search drives through minder locations,\\n[time: 96.7-98.82]  of course the source app leads to�니다.\\n[time: 99.36-100.8] ệt However,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'a937ab44-6d3a-4fb6-abd7-4e696b799ee6'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-82.74]  cognitive\\n[time: 82.74-96.7]  regulatory\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'd4b12c3f-403a-43a2-8c40-51a4c8b258f6'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"economics condition in Ireland in 2025\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m there\u001b[0m\u001b[33m is\u001b[0m\u001b[33m no\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m available\u001b[0m\u001b[33m about\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m are\u001b[0m\u001b[33m related\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m and\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mactive\u001b[0m\u001b[33m Architecture\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m),\u001b[0m\u001b[33m which\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m technology\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m and\u001b[0m\u001b[33m reliability\u001b[0m\u001b[33m of\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLarge\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Models\u001b[0m\u001b[33m).\n",
      "\n",
      "\u001b[0m\u001b[33mHowever\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m is\u001b[0m\u001b[33m possible\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m may\u001b[0m\u001b[33m be\u001b[0m\u001b[33m related\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m adoption\u001b[0m\u001b[33m and\u001b[0m\u001b[33m use\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m technology\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m country\u001b[0m\u001b[33m.\u001b[0m\u001b[33m If\u001b[0m\u001b[33m that\u001b[0m\u001b[33m is\u001b[0m\u001b[33m the\u001b[0m\u001b[33m case\u001b[0m\u001b[33m,\u001b[0m\u001b[33m then\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m may\u001b[0m\u001b[33m be\u001b[0m\u001b[33m influenced\u001b[0m\u001b[33m by\u001b[0m\u001b[33m factors\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m the\u001b[0m\u001b[33m cost\u001b[0m\u001b[33m of\u001b[0m\u001b[33m implementing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m maintaining\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m technology\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m availability\u001b[0m\u001b[33m of\u001b[0m\u001b[33m funding\u001b[0m\u001b[33m for\u001b[0m\u001b[33m research\u001b[0m\u001b[33m and\u001b[0m\u001b[33m development\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m economic\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m adopting\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m technology\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIt\u001b[0m\u001b[33m is\u001b[0m\u001b[33m also\u001b[0m\u001b[33m possible\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m may\u001b[0m\u001b[33m be\u001b[0m\u001b[33m influenced\u001b[0m\u001b[33m by\u001b[0m\u001b[33m other\u001b[0m\u001b[33m factors\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m the\u001b[0m\u001b[33m overall\u001b[0m\u001b[33m state\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economy\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m level\u001b[0m\u001b[33m of\u001b[0m\u001b[33m competition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m industry\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m for\u001b[0m\u001b[33m innovation\u001b[0m\u001b[33m and\u001b[0m\u001b[33m growth\u001b[0m\u001b[33m.\u001b[0m\u001b[33m However\u001b[0m\u001b[33m,\u001b[0m\u001b[33m without\u001b[0m\u001b[33m more\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m is\u001b[0m\u001b[33m difficult\u001b[0m\u001b[33m to\u001b[0m\u001b[33m say\u001b[0m\u001b[33m for\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m what\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m may\u001b[0m\u001b[33m be\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/agents/4e570e23-69b0-4aed-913b-fc311f643c67/session/cb5b26a1-7ee4-4538-80d6-a37a3ab67cd9 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant. Answer the user's question based only on the provided search results. Respond with 'I don’t know' if the information is outside of the scope of your knowledge and not present in the search results.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"asr-vector-db\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Name Red Hat RAG target audience and customers\",\n",
    "    \"Name key RAG benefits\",\n",
    "    \"Regular LLM output disadvantages\",\n",
    "    \"What is the economics condition at Ireland in 2025?\", # Dummy question the model will answer with 'I don’t know' or reason why can't answer \n",
    "]\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"prompt>\", prompt)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "# Get session response for further evaluation of RAG metrics\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=rag_agent.agent_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create .env.dev file and paste there your API Key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "with open(\".env.dev\", \"w\") as f:\n",
    "    f.write('GROQ_API_KEY=PASTE_YOUR_GROQ_API_KEY')\n",
    "\n",
    "# load env variable\n",
    "load_dotenv(dotenv_path=\".env.dev\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Union\n",
    "from llama_stack_client.types.agents import Turn\n",
    "\n",
    "# Compile regex pattern once for better performance\n",
    "CONTENT_PATTERN = re.compile(r\"Content:\\s*(.*?)(?=\\nMetadata:|$)\", re.DOTALL)\n",
    "\n",
    "# This function extracts the search results for the trace of each query\n",
    "def extract_retrieved_contexts(turn_object: Turn) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack tool execution responses.\n",
    "    \n",
    "    Args:\n",
    "        turn_object: A Turn object from LlamaStack containing steps with tool responses\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_context = []\n",
    "\n",
    "    # Filter tool execution steps first to reduce iterations\n",
    "    tool_steps = [step for step in turn_object.steps if step.step_type == \"tool_execution\"]\n",
    "    \n",
    "    for step in tool_steps:\n",
    "        for response in step.tool_responses:\n",
    "            if not response.content or not isinstance(response.content, list):\n",
    "                continue\n",
    "                \n",
    "            # Process all valid text items at once\n",
    "            text_items = [\n",
    "                item.text for item in response.content \n",
    "                if (hasattr(item, \"text\") and hasattr(item, \"type\") and \n",
    "                    item.type == \"text\" and item.text and \n",
    "                    item.text.startswith(\"Result \") and \"Content:\" in item.text)\n",
    "            ]\n",
    "            \n",
    "            # Extract content from all valid texts\n",
    "            for text in text_items:\n",
    "                match = CONTENT_PATTERN.search(text)\n",
    "                if match:\n",
    "                    retrieved_context.append(match.group(1).strip())\n",
    "\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List RAG key market use cases</td>\n",
       "      <td>[[time: 0.0-8.64]  Key market use cases. RAC i...</td>\n",
       "      <td>Based on the search results, the key market us...</td>\n",
       "      <td>\\nKey Market Use Cases\\nRAG is being adopted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name Red Hat RAG target audience and customers</td>\n",
       "      <td>[[time: 0.0-6.78]  Clarifying target audience ...</td>\n",
       "      <td>Based on the search results, the target audien...</td>\n",
       "      <td>\\nClarifying Target Audience and User Roles\\nT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user_input  \\\n",
       "0                   List RAG key market use cases   \n",
       "1  Name Red Hat RAG target audience and customers   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [[time: 0.0-8.64]  Key market use cases. RAC i...   \n",
       "1  [[time: 0.0-6.78]  Clarifying target audience ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the search results, the key market us...   \n",
       "1  Based on the search results, the target audien...   \n",
       "\n",
       "                                           reference  \n",
       "0  \\nKey Market Use Cases\\nRAG is being adopted a...  \n",
       "1  \\nClarifying Target Audience and User Roles\\nT...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = [\n",
    "'''\n",
    "Key Market Use Cases\n",
    "RAG is being adopted across various industries for diverse applications, including:\n",
    "\n",
    "Knowledge Question Answering: Providing accurate answers in customer service using product manuals or FAQs.\n",
    "\n",
    "Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "\n",
    "Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "\n",
    "Customer Service: Improving support accuracy with access to current product information.\n",
    "\n",
    "Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "\n",
    "Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "\n",
    "Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "\n",
    "General Task Assistance: Aiding users in various tasks requiring information access and decision-making.\n",
    "\n",
    "The rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption, allowing for tailored ad copy and product recommendations.\n",
    "''',\n",
    "    \n",
    "'''\n",
    "Clarifying Target Audience and User Roles\n",
    "This document clarifies the target audience and user roles for our project, focusing on the distinction between end-users and builders.\n",
    "End Users vs. Builders:\n",
    "\n",
    "End Users: Consume the final product (e.g., interact with a ChatGPT-like application).\n",
    "Builders: Create and configure the AI systems used by end-users (e.g., configure a RAG backend, tweaking parameters for a specific experience such as ChatGPT).  We are targeting builders, not end-users. Builders optimize their systems for their specific end-users.\n",
    "\n",
    "Builder Archetypes:\n",
    "\n",
    "High-Coder Builders (aka pro-code): Prefer SDKs and code-based solutions. They need access to all configurable parameters via APIs and SDKs.  They may also want a quick way to \"vibe check\" their RAG system via a UI (e.g., llama-stack-cli my-rag-app.py --web).\n",
    "\n",
    "Low-Coder Builders (no/low-code): Prefer UI-driven workflows and visual tools to configure their systems.  They could benefit from tools like the existing llama-stack playground.\n",
    "\n",
    "Builders vs. Platformers vs. Opsers:\n",
    "\n",
    "Builders (AI Engineers/AI Devs): Use the platform and its primitives to build AI systems.  Their skillset and the complexity of their tasks determine whether they are considered AI Engineers or AI Devs.\n",
    "\n",
    "Platformers (AI Platform Engineers): Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve both Builders (for development) and Opsers (for deployment/operations), ensuring infrastructure is reliable, scalable, and supports self-service.\n",
    "\n",
    "Opsers (AI/MLOps Engineers): Opsers focus on operationalizing and automating the AI/ML  lifecycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling Builders' models to reach and succeed in production. Opsers work closely with Platformers to ensure infrastructure meets operational needs.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Platformers enable builders, and builders create systems for end-users.  Our focus is on empowering builders with the tools and flexibility they need to build the best experiences for their end-users.''',\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, turn in enumerate(session_response.turns[:2]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": turn.input_messages[0].content,\n",
    "            \"response\": turn.output_message.content,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(turn),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ibm-granite/granite-embedding-125m-english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved contexts for the first prompt: ['[time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-82.74]  cognitive\\n[time: 82.74-96.7]  regulatory', \"[time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\", '[time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-86.2]  Marjorie\\n[time: 86.2-93.53999999999999]  Notre\\n[time: 93.53999999999999-95.62]  Hay\\n[time: 95.62-96.9]  Cha\\n[time: 96.9-100.1]  Cha', '[time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.67999999999999]  Legal applications.\\n[time: 80.67999999999999-81.66]  At this time,\\n[time: 81.66-90.36]  of course,\\n[time: 90.96-95.98]  deep search drives through minder locations,\\n[time: 96.7-98.82]  of course the source app leads to�니다.\\n[time: 99.36-100.8] ệt However,', '[time: 109.0-118.0]  For developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\n[time: 118.0-122.0]  Data sovereignty and privacy\\n[time: 122.0-131.0]  Using confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\n[time: 131.0-143.0]  RAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\n[time: 143.0-151.0]  RAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\n[time: 151.0-158.0]  That is, certain users can access certain information based on their security clearance levels.\\n[time: 158.0-159.0]  From friendly to aggregate to a type of research, rituals ideas?\\n[time: 159.0-163.0]  So, once a application arrives there is RAC.\\n[time: 163.0-168.0]  Overups are really accurate, it takes time to find things threshold.\\n[time: 168.0-170.66]  That is a test目 of naming that the data feature that those partners in the center stands will come out.\\n[time: 171.0-173.0]  Alright, guys, so what do you make throughout this area?\\n[time: 174.0-176.0]  Let us hear that, for sure, what are the dec Mariotaetri....\\n[time: 177.0-178.5]  ...through Abgah the Aliya ?\\n[time: 179.0-181.0]  All right.\\n[time: 181.0-182.5]  You can find the STOP.\\n[time: 183.0-185.5]  I assume that, perfectly happy that we hold back over here never wasting all the data.']\n",
      "\n",
      "Retrieved contexts for the second prompt: ['[time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a ChatGPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as ChatGPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.', '[time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.', '[time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.', '[time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.', \"[time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-75.76]  of our paliplades.\\n[time: 76.34-85.46]  Along the skew formation, we have tailored Honestly, it is a reliable\\n[time: 85.53999999999999-102.96]  use of this person making money. We want to utilize the balance. We'd get an S dealt with\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness, \n",
    "    ResponseRelevancy,\n",
    ") \n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [line.strip() for line in reference_for_first_prompt.strip().split('\\n')]\n",
    "reference_list_for_second_prompt = [line.strip() for line in reference_for_second_prompt.strip().split('\\n')]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\")\n",
    "print(f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'List RAG key market use cases': 1.0\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 40.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'Name Red Hat RAG target audience and customers': 0.8\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'List RAG key market use cases': 0.9245535248525503\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "response_relevancy_score_for_first_prompt = await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'Name Red Hat RAG target audience and customers': 0.9862592839954273\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "response_relevancy_score_for_second_prompt = await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
