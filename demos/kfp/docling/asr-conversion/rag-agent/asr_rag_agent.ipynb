{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models information: [Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "\n",
      "Identifier for Inference model in usage: vllm\n",
      "\n",
      "=== Available Vector Databases ===\n",
      "- ID: my-asr-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: granite-embedding-125m\n",
      "\n",
      "- ID: audio-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: granite-embedding-125m\n",
      "\n",
      "- ID: asr-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: granite-embedding-125m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next((model.identifier for model in models if model.model_type == 'llm'), None)\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create RAG Agent and prompt the LLM\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-76.5]  Self- clairvoyant innovations.\\n[time: 76.5-98.58]  If referral costs are 곱 눌러ty, now tool makes information available to\\n[time: 98.58-100.58] itzerland roadmap development, as 민� pails are whats with performance. My number has exercised\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '054f609a-3cee-4fda-9447-b9a56349b4ed'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-81.78]  A\\n[time: 81.78-91.42]  human linguistic\\n[time: 93.03999999999999-102.88]  F-\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '14b93a6d-5eb2-461b-8d3b-df49d9c49241'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-81.58]  Legal applications,\\n[time: 81.7-82.82]  Zipo命,\\n[time: 82.82-88.8]  2.\\n[time: 88.8-94.66]  Switch account economically products.\\n[time: 94.66-95.68]  No doubt,\\n[time: 95.68-97.18]  You know,\\n[time: 97.56-98.53999999999999]  What evidence do you know\\n[time: 98.53999999999999-99.44]  Is there any Chipotle\\n[time: 99.44-99.48]  inâl Ś Мар abbrevi\\n[time: 99.48-100.68]  or\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '4c25544d-3333-4d6b-8380-06f563d989b0'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 1.92-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.72-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  cookie features.\\n[time: 75.66-75.8]  Check out the information that we have provided by data looking at First Federal Accounting\\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-76.94]  As Hello to read the conversations.\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '3a70caab-732c-44e5-ac59-ab256a137d43'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 1.92-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.72-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-76.74]  underpreRL corporations, offerings by Thanks to All-Sцион R practiced and marshmallow scissors to\\n[time: 76.74-77.54]  aimize trading. Sono car器 Além fuists in precise programs and documentation spiritual\\n[time: 77.54-77.54] \\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '25faec15-8bf9-4f95-81d6-a1617a897991'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mlevance\u001b[0m\u001b[33m-A\u001b[0m\u001b[33mware\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m)\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m using\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m fax\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m retrieving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m enhancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m improving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m,\u001b[0m\u001b[33m enabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-h\u001b[0m\u001b[33mub\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m,\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m retrieving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m task\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m,\u001b[0m\u001b[33m aiding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m making\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m and\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m more\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Name Red Hat RAG target audience and customers\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Red Hat RAG target audience and customers'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 1.7600000000000002-6.76]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.76-13.4]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 14.18-18.58]  End users vs. builders. End users consume the final product.\\n[time: 19.42-26.2]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.2-33.24]  used by end users. Example. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.24-39.46]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.46-49.18]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.18-55.92]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.92-64.04]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.04-72.66]  system via a UI. Example. LamaStackCliMyRackApp.py-web.\\n[time: 75.56-83.84]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.84-90.8]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 92.76-103.04]  Builders vs. Platformers vs. Opsers. Builders, AI engineers and AI devs. They use the platform\\n[time: 103.04-109.62]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.62-118.86]  whether they are considered AI engineers or AI devs. Platformers. AI platform engineers.\\n[time: 119.04-125.7]  Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '0c6c7834-0417-4a15-affc-15dba7dbf762'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 1.7000000000000015-6.76]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.76-13.4]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 14.12-18.58]  End users vs. builders. End users consume the final product.\\n[time: 19.360000000000003-26.2]  Interact with a ChatGPT-like application. Builders create and configure the AI systems\\n[time: 26.2-33.24]  used by end users. Example. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.24-39.46]  experience such as ChatGPT. We are targeting builders not end users. Builders optimize\\n[time: 39.46-49.16]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.16-55.92]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.92-64.04]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.04-72.66]  system via a UI. Example. LamaStackCliMyRackApp.py-web.\\n[time: 75.56-83.84]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.84-90.8]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 92.76-103.04]  Builders vs. Platformers vs. Opsers. Builders, AI engineers and AI devs. They use the platform\\n[time: 103.04-109.62]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.62-118.86]  whether they are considered AI engineers or AI devs. Platformers. AI platform engineers.\\n[time: 119.04-125.7]  Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'f42431dc-bb0e-45e0-a511-40e8669e9d4b'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 1.7000000000000015-6.76]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.76-13.4]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 14.12-18.58]  End users vs. builders. End users consume the final product.\\n[time: 19.360000000000003-26.2]  Interact with a ChatGPT-like application. Builders create and configure the AI systems\\n[time: 26.2-33.24]  used by end users. Example. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.24-39.46]  experience such as ChatGPT. We are targeting builders not end users. Builders optimize\\n[time: 39.46-49.16]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.16-55.92]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.92-64.04]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.04-72.66]  system via a UI. Example. LamaStackCliMyRackApp.py-web.\\n[time: 75.56-83.84]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.84-90.8]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 92.76-103.04]  Builders vs. Platformers vs. Opsers. Builders, AI engineers and AI devs. They use the platform\\n[time: 103.04-109.62]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.62-118.86]  whether they are considered AI engineers or AI devs. Platformers. AI platform engineers.\\n[time: 119.04-125.7]  Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '19a8ebd5-a9da-447f-9653-be2ed78b2a9a'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-6.78]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.78-13.44]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 13.44-18.6]  End users vs. builders. End users consume the final product.\\n[time: 18.6-26.22]  Interact with a ChatGPT-like application. Builders create and configure the AI systems\\n[time: 26.22-33.3]  used by end users. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.3-39.54]  experience such as ChatGPT. We are targeting builders not end users. Builders optimize\\n[time: 39.54-49.2]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.2-55.8]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.8-64.02]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.02-72.53999999999999]  system via a UI. Example, LamaStackCliMyRackApp.py-web.\\n[time: 72.53999999999999-83.82]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.82-93.03999999999999]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 93.03999999999999-103.25999999999999]  Builders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\n[time: 103.25999999999999-109.82]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.82-119.03999999999999]  whether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '7f848af1-4b4d-4497-ae78-fcf316e0f393'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-76.5]  Self- clairvoyant innovations.\\n[time: 76.5-98.58]  If referral costs are 곱 눌러ty, now tool makes information available to\\n[time: 98.58-100.58] itzerland roadmap development, as 민� pails are whats with performance. My number has exercised\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '054f609a-3cee-4fda-9447-b9a56349b4ed'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Red Hat RAG target audience and customers\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m and\u001b[0m\u001b[33m customers\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Builders\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m create\u001b[0m\u001b[33m and\u001b[0m\u001b[33m configure\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m used\u001b[0m\u001b[33m by\u001b[0m\u001b[33m end\u001b[0m\u001b[33m users\u001b[0m\u001b[33m.\u001b[0m\u001b[33m They\u001b[0m\u001b[33m are\u001b[0m\u001b[33m divided\u001b[0m\u001b[33m into\u001b[0m\u001b[33m two\u001b[0m\u001b[33m categories\u001b[0m\u001b[33m:\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m+\u001b[0m\u001b[33m High\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m (\u001b[0m\u001b[33mPro\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m):\u001b[0m\u001b[33m They\u001b[0m\u001b[33m prefer\u001b[0m\u001b[33m SDK\u001b[0m\u001b[33ms\u001b[0m\u001b[33m and\u001b[0m\u001b[33m code\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m solutions\u001b[0m\u001b[33m and\u001b[0m\u001b[33m need\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m all\u001b[0m\u001b[33m configurable\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m via\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m and\u001b[0m\u001b[33m SDK\u001b[0m\u001b[33ms\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m+\u001b[0m\u001b[33m Low\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m prefer\u001b[0m\u001b[33m UI\u001b[0m\u001b[33m-driven\u001b[0m\u001b[33m workflows\u001b[0m\u001b[33m and\u001b[0m\u001b[33m visual\u001b[0m\u001b[33m tools\u001b[0m\u001b[33m to\u001b[0m\u001b[33m configure\u001b[0m\u001b[33m their\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33mers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m focus\u001b[0m\u001b[33m on\u001b[0m\u001b[33m building\u001b[0m\u001b[33m,\u001b[0m\u001b[33m maintaining\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m securing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m Engineers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m primitives\u001b[0m\u001b[33m to\u001b[0m\u001b[33m build\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m Dev\u001b[0m\u001b[33ms\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m primitives\u001b[0m\u001b[33m to\u001b[0m\u001b[33m build\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Ops\u001b[0m\u001b[33mers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m They\u001b[0m\u001b[33m focus\u001b[0m\u001b[33m on\u001b[0m\u001b[33m building\u001b[0m\u001b[33m,\u001b[0m\u001b[33m maintaining\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m securing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m exact\u001b[0m\u001b[33m definitions\u001b[0m\u001b[33m and\u001b[0m\u001b[33m categor\u001b[0m\u001b[33mizations\u001b[0m\u001b[33m of\u001b[0m\u001b[33m these\u001b[0m\u001b[33m roles\u001b[0m\u001b[33m may\u001b[0m\u001b[33m vary\u001b[0m\u001b[33m depending\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m context\u001b[0m\u001b[33m and\u001b[0m\u001b[33m industry\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Name key RAG benefits\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG benefits'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-76.5]  Self- clairvoyant innovations.\\n[time: 76.5-98.58]  If referral costs are 곱 눌러ty, now tool makes information available to\\n[time: 98.58-100.58] itzerland roadmap development, as 민� pails are whats with performance. My number has exercised\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '054f609a-3cee-4fda-9447-b9a56349b4ed'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 1.9200000000000008-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.74-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  Technology re- warnedawanese, Independent benefits and Hispanic participants for\\n[time: 75.66-75.66] \\n[time: 75.66-75.76]  questions.\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '35b4a9cf-a9a2-48c6-996f-4f03d5a6f2bc'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-81.58]  Legal applications,\\n[time: 81.7-82.82]  Zipo命,\\n[time: 82.82-88.8]  2.\\n[time: 88.8-94.66]  Switch account economically products.\\n[time: 94.66-95.68]  No doubt,\\n[time: 95.68-97.18]  You know,\\n[time: 97.56-98.53999999999999]  What evidence do you know\\n[time: 98.53999999999999-99.44]  Is there any Chipotle\\n[time: 99.44-99.48]  inâl Ś Мар abbrevi\\n[time: 99.48-100.68]  or\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '4c25544d-3333-4d6b-8380-06f563d989b0'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-81.78]  A\\n[time: 81.78-91.42]  human linguistic\\n[time: 93.03999999999999-102.88]  F-\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '14b93a6d-5eb2-461b-8d3b-df49d9c49241'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 1.92-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.72-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  cookie features.\\n[time: 75.66-75.8]  Check out the information that we have provided by data looking at First Federal Accounting\\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-75.8] \\n[time: 75.8-76.94]  As Hello to read the conversations.\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '3a70caab-732c-44e5-ac59-ab256a137d43'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG benefits\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mlevance\u001b[0m\u001b[33m-A\u001b[0m\u001b[33mware\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m)\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m using\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m fax\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m retrieving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m enhancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m improving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m,\u001b[0m\u001b[33m enabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-h\u001b[0m\u001b[33mub\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m,\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m retrieving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m task\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m,\u001b[0m\u001b[33m aiding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m making\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m and\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m more\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Regular LLM output disadvantages\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Regular LLM output disadvantages'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-5.0]  RAC vs. Regular LLM Outputs\\n[time: 5.0-13.0]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.0-24.0]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.0-30.0]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.0-40.0]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 40.0-48.0]  LLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 48.0-54.0]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 54.0-63.0]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 63.0-75.0]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.0-87.0]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 87.0-98.12]  Therebyby makes\\n[time: 98.12-103.0]  in the process of getting into account a short- Omaha sealant on search for the data that travels with area that can change riser in our size.\\n[time: 103.0-108.0]  Liz Herman Trelleson- nas from Arizona Serresia tours\\n[time: 108.0-110.62]  Her paper Hazel Kel util t-manns is\\nMetadata: {'file_name': 'tmp2_6t3kcq_RAG_vs_Regular_LLM_Output', 'document_id': '3dc65fa3-6f95-4a13-b02d-19c65fe88257'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 2.0600000000000005-4.94]  RAC vs. Regular LLM Outputs\\n[time: 4.94-13.42]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.72-23.58]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.52-29.78]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.34-39.58]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 41.1-47.58]  LLMs have a knowledge cutoff date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 47.7-54.26]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 55.32-62.96]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 64.34-75.4]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.4-86.3]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 86.3-86.3] \\n[time: 86.3-88.16]  I will amplify and provide it with caution.\\n[time: 88.16-88.16] \\n[time: 88.16-88.16] \\n[time: 88.16-88.16] \\n[time: 88.16-88.16] \\n[time: 88.16-88.16] \\n[time: 88.14-88.14] \\nMetadata: {'file_name': 'tmpqe3jer4v_RAG_vs_Regular_LLM_Output', 'document_id': 'aea6b6b9-326b-4312-9681-f45aadd64aa0'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 2.0600000000000005-4.94]  RAC vs. Regular LLM Outputs\\n[time: 4.94-13.42]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.72-23.58]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.52-29.78]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.34-39.58]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 41.1-47.58]  LLMs have a knowledge cutoff date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 47.7-54.26]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 55.32-62.96]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 64.34-75.4]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.4-86.3]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 86.3-86.32]  In the phonetic application process, including משidän\\n[time: 86.32-86.32] \\n[time: 86.32-86.4]  And theiling life of abuse is designed with aunion-based question,- quite definitely.\\n[time: 87.24000000000001-88.16]  For example, other issues Tak pieces-basedว sven.\\n[time: 88.14-88.14] \\nMetadata: {'file_name': 'tmpvd7tu272_RAG_vs_Regular_LLM_Output', 'document_id': 'f30fc434-f775-4897-96ba-a611b3df621d'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 0.0-3.0]  The benefits of RAC\\n[time: 3.0-12.0]  The retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\n[time: 12.0-21.0]  Grounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\n[time: 21.0-22.0]  Accuracy\\n[time: 22.0-27.0]  RAC provides an LLM with sources it can cite so users can verify these claims.\\n[time: 27.0-34.0]  You can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\n[time: 34.0-44.0]  Overall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\n[time: 44.0-46.0]  Cost effectiveness\\n[time: 46.0-56.0]  Retraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\n[time: 56.0-66.0]  With RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\n[time: 66.0-70.0]  RAC can also reduce inference codes.\\n[time: 70.0-84.0]  LLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\n[time: 84.0-97.0]  Rather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\n[time: 97.0-99.0]  Developer Control\\n[time: 99.0-109.0]  Compared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nMetadata: {'file_name': 'tmpqf7c9n3l_RAG_benefits', 'document_id': '2b840c07-0414-4e11-a260-736b63387453'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 1.1000000000000003-3.02]  The benefits of RAC\\n[time: 3.02-12.46]  The retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\n[time: 12.78-21.04]  Grounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\n[time: 21.840000000000003-22.0]  Accuracy\\n[time: 22.6-27.44]  RAC provides an LLM with sources it can cite so users can verify these claims.\\n[time: 27.44-34.5]  You can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\n[time: 35.1-43.42]  Overall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\n[time: 44.6-45.34]  Cost effectiveness\\n[time: 45.34-56.2]  Retraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\n[time: 56.2-66.58]  With RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\n[time: 67.08-69.96]  RAC can also reduce inference codes.\\n[time: 69.96-83.7]  LLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\n[time: 83.7-96.8]  Rather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\n[time: 97.34-99.2]  Developer Control\\n[time: 99.83999999999999-108.12]  Compared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nMetadata: {'file_name': 'tmp_37sgp4v_RAG_benefits', 'document_id': '99617c55-ab42-4556-a600-4c5098356aee'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Regular LLM output disadvantages\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m of\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLarge\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Model\u001b[0m\u001b[33m)\u001b[0m\u001b[33m output\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m are\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m with\u001b[0m\u001b[33m generally\u001b[0m\u001b[33m available\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m might\u001b[0m\u001b[33m not\u001b[0m\u001b[33m include\u001b[0m\u001b[33m the\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m you\u001b[0m\u001b[33m want\u001b[0m\u001b[33m them\u001b[0m\u001b[33m to\u001b[0m\u001b[33m reference\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m cutoff\u001b[0m\u001b[33m date\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m have\u001b[0m\u001b[33m a\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m cutoff\u001b[0m\u001b[33m date\u001b[0m\u001b[33m,\u001b[0m\u001b[33m meaning\u001b[0m\u001b[33m the\u001b[0m\u001b[33m information\u001b[0m\u001b[33m they\u001b[0m\u001b[33m've\u001b[0m\u001b[33m been\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m doesn\u001b[0m\u001b[33m't\u001b[0m\u001b[33m continuously\u001b[0m\u001b[33m gather\u001b[0m\u001b[33m updates\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Out\u001b[0m\u001b[33mdated\u001b[0m\u001b[33m information\u001b[0m\u001b[33m:\u001b[0m\u001b[33m As\u001b[0m\u001b[33m a\u001b[0m\u001b[33m result\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m resource\u001b[0m\u001b[33m material\u001b[0m\u001b[33m can\u001b[0m\u001b[33m become\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m and\u001b[0m\u001b[33m no\u001b[0m\u001b[33m longer\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m False\u001b[0m\u001b[33m or\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m information\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m are\u001b[0m\u001b[33m eager\u001b[0m\u001b[33m to\u001b[0m\u001b[33m please\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m means\u001b[0m\u001b[33m they\u001b[0m\u001b[33m sometimes\u001b[0m\u001b[33m present\u001b[0m\u001b[33m false\u001b[0m\u001b[33m or\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m also\u001b[0m\u001b[33m known\u001b[0m\u001b[33m as\u001b[0m\u001b[33m halluc\u001b[0m\u001b[33mination\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Exp\u001b[0m\u001b[33mensive\u001b[0m\u001b[33m inference\u001b[0m\u001b[33m codes\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m queries\u001b[0m\u001b[33m are\u001b[0m\u001b[33m expensive\u001b[0m\u001b[33m,\u001b[0m\u001b[33m placing\u001b[0m\u001b[33m demands\u001b[0m\u001b[33m on\u001b[0m\u001b[33m your\u001b[0m\u001b[33m own\u001b[0m\u001b[33m hardware\u001b[0m\u001b[33m if\u001b[0m\u001b[33m you\u001b[0m\u001b[33m run\u001b[0m\u001b[33m a\u001b[0m\u001b[33m local\u001b[0m\u001b[33m model\u001b[0m\u001b[33m or\u001b[0m\u001b[33m running\u001b[0m\u001b[33m up\u001b[0m\u001b[33m a\u001b[0m\u001b[33m mattered\u001b[0m\u001b[33m bill\u001b[0m\u001b[33m if\u001b[0m\u001b[33m you\u001b[0m\u001b[33m use\u001b[0m\u001b[33m an\u001b[0m\u001b[33m external\u001b[0m\u001b[33m service\u001b[0m\u001b[33m through\u001b[0m\u001b[33m an\u001b[0m\u001b[33m application\u001b[0m\u001b[33m programming\u001b[0m\u001b[33m interface\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Large\u001b[0m\u001b[33m query\u001b[0m\u001b[33m size\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Rather\u001b[0m\u001b[33m than\u001b[0m\u001b[33m sending\u001b[0m\u001b[33m an\u001b[0m\u001b[33m entire\u001b[0m\u001b[33m reference\u001b[0m\u001b[33m document\u001b[0m\u001b[33m to\u001b[0m\u001b[33m an\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m at\u001b[0m\u001b[33m once\u001b[0m\u001b[33m,\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m can\u001b[0m\u001b[33m send\u001b[0m\u001b[33m only\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m reference\u001b[0m\u001b[33m material\u001b[0m\u001b[33m,\u001b[0m\u001b[33m thereby\u001b[0m\u001b[33m reducing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m size\u001b[0m\u001b[33m of\u001b[0m\u001b[33m queries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m improving\u001b[0m\u001b[33m efficiency\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m developer\u001b[0m\u001b[33m control\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m traditional\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m,\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m accessible\u001b[0m\u001b[33m and\u001b[0m\u001b[33m straightforward\u001b[0m\u001b[33m way\u001b[0m\u001b[33m to\u001b[0m\u001b[33m get\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m,\u001b[0m\u001b[33m troub\u001b[0m\u001b[33mleshoot\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fix\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m limitations\u001b[0m\u001b[33m and\u001b[0m\u001b[33m challenges\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m how\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mlevance\u001b[0m\u001b[33m-A\u001b[0m\u001b[33mware\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m)\u001b[0m\u001b[33m can\u001b[0m\u001b[33m address\u001b[0m\u001b[33m these\u001b[0m\u001b[33m issues\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m,\u001b[0m\u001b[33m efficient\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m cost\u001b[0m\u001b[33m-effective\u001b[0m\u001b[33m solution\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> What is the economics condition at Ireland in 2025?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'economics condition in Ireland in 2025'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-76.5]  Self- clairvoyant innovations.\\n[time: 76.5-98.58]  If referral costs are 곱 눌러ty, now tool makes information available to\\n[time: 98.58-100.58] itzerland roadmap development, as 민� pails are whats with performance. My number has exercised\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '054f609a-3cee-4fda-9447-b9a56349b4ed'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: [time: 1.92-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.72-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-76.74]  underpreRL corporations, offerings by Thanks to All-Sцион R practiced and marshmallow scissors to\\n[time: 76.74-77.54]  aimize trading. Sono car器 Além fuists in precise programs and documentation spiritual\\n[time: 77.54-77.54] \\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '25faec15-8bf9-4f95-81d6-a1617a897991'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: [time: 0.0-8.64]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.64-15.24]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.24-21.3]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.3-28.080000000000002]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.08-35.16]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.4-41.54]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.56-48.379999999999995]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.22-54.66]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.66-61.94]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.94-68.06]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.06-75.58]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.58-81.58]  Legal applications,\\n[time: 81.7-82.82]  Zipo命,\\n[time: 82.82-88.8]  2.\\n[time: 88.8-94.66]  Switch account economically products.\\n[time: 94.66-95.68]  No doubt,\\n[time: 95.68-97.18]  You know,\\n[time: 97.56-98.53999999999999]  What evidence do you know\\n[time: 98.53999999999999-99.44]  Is there any Chipotle\\n[time: 99.44-99.48]  inâl Ś Мар abbrevi\\n[time: 99.48-100.68]  or\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '4c25544d-3333-4d6b-8380-06f563d989b0'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: [time: 1.9200000000000008-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.74-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  Technology re- warnedawanese, Independent benefits and Hispanic participants for\\n[time: 75.66-75.66] \\n[time: 75.66-75.76]  questions.\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '35b4a9cf-a9a2-48c6-996f-4f03d5a6f2bc'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: [time: 76.94-77.52] LAUGHING To remind everybody, the網賴omi doesn't limit the importance of the\\n[time: 77.52-77.52] \\n[time: 77.52-77.52] \\n[time: 77.52-77.52] \\n[time: 77.52-77.52] \\n[time: 77.52-77.54]  Thank you.\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': 'b89d9b50-b0a5-49a7-9725-4fbc2e88bf4b'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"economics condition in Ireland in 2025\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mUnfortunately\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m do\u001b[0m\u001b[33m not\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m any\u001b[0m\u001b[33m information\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economics\u001b[0m\u001b[33m condition\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m results\u001b[0m\u001b[33m are\u001b[0m\u001b[33m related\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m and\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mlevance\u001b[0m\u001b[33m-A\u001b[0m\u001b[33mware\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m)\u001b[0m\u001b[33m and\u001b[0m\u001b[33m do\u001b[0m\u001b[33m not\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m any\u001b[0m\u001b[33m data\u001b[0m\u001b[33m or\u001b[0m\u001b[33m insights\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economic\u001b[0m\u001b[33m conditions\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Therefore\u001b[0m\u001b[33m,\u001b[0m\u001b[33m I\u001b[0m\u001b[33m cannot\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m a\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m answer\u001b[0m\u001b[33m to\u001b[0m\u001b[33m this\u001b[0m\u001b[33m question\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/agents/2f586268-3a90-476c-80ab-2097921ffb9e/session/49531532-e236-4eec-8253-a760f6c6ca75 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant. Answer the user's question based only on the provided search results. Respond with 'I don’t know' if the information is outside of the scope of your knowledge and not present in the search results.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"asr-vector-db\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Name Red Hat RAG target audience and customers\",\n",
    "    \"Name key RAG benefits\",\n",
    "    \"Regular LLM output disadvantages\",\n",
    "    \"What is the economics condition at Ireland in 2025?\", # Dummy question the model will answer with 'I don’t know' \n",
    "]\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"prompt>\", prompt)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "# Get session response for further evaluation of RAG metrics\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=rag_agent.agent_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create .env.dev file and paste there your API Key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "with open(\".env.dev\", \"w\") as f:\n",
    "    f.write('GROQ_API_KEY=PASTE_YOUR_GROQ_API_KEY')\n",
    "\n",
    "# load env variable\n",
    "load_dotenv(dotenv_path=\".env.dev\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Union\n",
    "from llama_stack_client.types.agents import Turn\n",
    "\n",
    "# Compile regex pattern once for better performance\n",
    "CONTENT_PATTERN = re.compile(r\"Content:\\s*(.*?)(?=\\nMetadata:|$)\", re.DOTALL)\n",
    "\n",
    "# This function extracts the search results for the trace of each query\n",
    "def extract_retrieved_contexts(turn_object: Turn) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack tool execution responses.\n",
    "    \n",
    "    Args:\n",
    "        turn_object: A Turn object from LlamaStack containing steps with tool responses\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_context = []\n",
    "\n",
    "    # Filter tool execution steps first to reduce iterations\n",
    "    tool_steps = [step for step in turn_object.steps if step.step_type == \"tool_execution\"]\n",
    "    \n",
    "    for step in tool_steps:\n",
    "        for response in step.tool_responses:\n",
    "            if not response.content or not isinstance(response.content, list):\n",
    "                continue\n",
    "                \n",
    "            # Process all valid text items at once\n",
    "            text_items = [\n",
    "                item.text for item in response.content \n",
    "                if (hasattr(item, \"text\") and hasattr(item, \"type\") and \n",
    "                    item.type == \"text\" and item.text and \n",
    "                    item.text.startswith(\"Result \") and \"Content:\" in item.text)\n",
    "            ]\n",
    "            \n",
    "            # Extract content from all valid texts\n",
    "            for text in text_items:\n",
    "                match = CONTENT_PATTERN.search(text)\n",
    "                if match:\n",
    "                    retrieved_context.append(match.group(1).strip())\n",
    "\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List RAG key market use cases</td>\n",
       "      <td>[[time: 0.0-8.64]  Key market use cases. RAC i...</td>\n",
       "      <td>Based on the search results, the key market us...</td>\n",
       "      <td>\\nKey Market Use Cases\\nRAG is being adopted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name Red Hat RAG target audience and customers</td>\n",
       "      <td>[[time: 1.7600000000000002-6.76]  Clarifying t...</td>\n",
       "      <td>Based on the search results, the target audien...</td>\n",
       "      <td>\\nClarifying Target Audience and User Roles\\nT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user_input  \\\n",
       "0                   List RAG key market use cases   \n",
       "1  Name Red Hat RAG target audience and customers   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [[time: 0.0-8.64]  Key market use cases. RAC i...   \n",
       "1  [[time: 1.7600000000000002-6.76]  Clarifying t...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the search results, the key market us...   \n",
       "1  Based on the search results, the target audien...   \n",
       "\n",
       "                                           reference  \n",
       "0  \\nKey Market Use Cases\\nRAG is being adopted a...  \n",
       "1  \\nClarifying Target Audience and User Roles\\nT...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = [\n",
    "'''\n",
    "Key Market Use Cases\n",
    "RAG is being adopted across various industries for diverse applications, including:\n",
    "\n",
    "Knowledge Question Answering: Providing accurate answers in customer service using product manuals or FAQs.\n",
    "\n",
    "Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "\n",
    "Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "\n",
    "Customer Service: Improving support accuracy with access to current product information.\n",
    "\n",
    "Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "\n",
    "Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "\n",
    "Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "\n",
    "General Task Assistance: Aiding users in various tasks requiring information access and decision-making.\n",
    "\n",
    "The rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption, allowing for tailored ad copy and product recommendations.\n",
    "''',\n",
    "    \n",
    "'''\n",
    "Clarifying Target Audience and User Roles\n",
    "This document clarifies the target audience and user roles for our project, focusing on the distinction between end-users and builders.\n",
    "End Users vs. Builders:\n",
    "\n",
    "End Users: Consume the final product (e.g., interact with a ChatGPT-like application).\n",
    "Builders: Create and configure the AI systems used by end-users (e.g., configure a RAG backend, tweaking parameters for a specific experience such as ChatGPT).  We are targeting builders, not end-users. Builders optimize their systems for their specific end-users.\n",
    "\n",
    "Builder Archetypes:\n",
    "\n",
    "High-Coder Builders (aka pro-code): Prefer SDKs and code-based solutions. They need access to all configurable parameters via APIs and SDKs.  They may also want a quick way to \"vibe check\" their RAG system via a UI (e.g., llama-stack-cli my-rag-app.py --web).\n",
    "\n",
    "Low-Coder Builders (no/low-code): Prefer UI-driven workflows and visual tools to configure their systems.  They could benefit from tools like the existing llama-stack playground.\n",
    "\n",
    "Builders vs. Platformers vs. Opsers:\n",
    "\n",
    "Builders (AI Engineers/AI Devs): Use the platform and its primitives to build AI systems.  Their skillset and the complexity of their tasks determine whether they are considered AI Engineers or AI Devs.\n",
    "\n",
    "Platformers (AI Platform Engineers): Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve both Builders (for development) and Opsers (for deployment/operations), ensuring infrastructure is reliable, scalable, and supports self-service.\n",
    "\n",
    "Opsers (AI/MLOps Engineers): Opsers focus on operationalizing and automating the AI/ML  lifecycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling Builders' models to reach and succeed in production. Opsers work closely with Platformers to ensure infrastructure meets operational needs.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Platformers enable builders, and builders create systems for end-users.  Our focus is on empowering builders with the tools and flexibility they need to build the best experiences for their end-users.''',\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, turn in enumerate(session_response.turns[:2]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": turn.input_messages[0].content,\n",
    "            \"response\": turn.output_message.content,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(turn),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ibm-granite/granite-embedding-125m-english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved contexts for the first prompt: ['[time: 1.9200000000000008-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.74-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  Technology re- warnedawanese, Independent benefits and Hispanic participants for\\n[time: 75.66-75.66] \\n[time: 75.66-75.76]  questions.', \"[time: 108.42-117.48]  For developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\n[time: 117.48-121.24]  Data sovereignty and privacy\\n[time: 121.24-131.32]  Using confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\n[time: 131.32-142.92]  RAC offers a solution to privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\n[time: 143.06-150.32]  RAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\n[time: 151.08-157.8]  That is, certain users can access certain information based on their security clearance levels.\\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.94]  It's another region, but from the difference in the reactivity field, the electrical data – Leo Meto is reaching to the Maininary leaf host.\\n[time: 157.94-157.94]\", \"[time: 1.1000000000000003-3.02]  The benefits of RAC\\n[time: 3.02-12.46]  The retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\n[time: 12.78-21.04]  Grounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\n[time: 21.840000000000003-22.0]  Accuracy\\n[time: 22.6-27.44]  RAC provides an LLM with sources it can cite so users can verify these claims.\\n[time: 27.44-34.5]  You can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\n[time: 35.1-43.42]  Overall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\n[time: 44.6-45.34]  Cost effectiveness\\n[time: 45.34-56.2]  Retraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\n[time: 56.2-66.58]  With RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\n[time: 67.08-69.96]  RAC can also reduce inference codes.\\n[time: 69.96-83.7]  LLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\n[time: 83.7-96.8]  Rather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\n[time: 97.34-99.2]  Developer Control\\n[time: 99.83999999999999-108.12]  Compared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\", \"[time: 2.0600000000000005-4.94]  RAC vs. Regular LLM Outputs\\n[time: 4.94-13.42]  LLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\n[time: 13.72-23.58]  AI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\n[time: 24.52-29.78]  LLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\n[time: 30.34-39.58]  LLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\n[time: 41.1-47.58]  LLMs have a knowledge cutoff date, meaning the information they've been trained on doesn't continuously gather updates.\\n[time: 47.7-54.26]  As a result, the resource material can become outdated and no longer relevant.\\n[time: 55.32-62.96]  LLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\n[time: 64.34-75.4]  Implementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\n[time: 75.4-86.3]  The LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\n[time: 86.3-86.32]  In the phonetic application process, including משidän\\n[time: 86.32-86.32] \\n[time: 86.32-86.4]  And theiling life of abuse is designed with aunion-based question,- quite definitely.\\n[time: 87.24000000000001-88.16]  For example, other issues Tak pieces-basedว sven.\\n[time: 88.14-88.14]\", '[time: 1.7600000000000002-6.76]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.76-13.4]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 14.18-18.58]  End users vs. builders. End users consume the final product.\\n[time: 19.42-26.2]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.2-33.24]  used by end users. Example. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.24-39.46]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.46-49.18]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.18-55.92]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.92-64.04]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.04-72.66]  system via a UI. Example. LamaStackCliMyRackApp.py-web.\\n[time: 75.56-83.84]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.84-90.8]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 92.76-103.04]  Builders vs. Platformers vs. Opsers. Builders, AI engineers and AI devs. They use the platform\\n[time: 103.04-109.62]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.62-118.86]  whether they are considered AI engineers or AI devs. Platformers. AI platform engineers.\\n[time: 119.04-125.7]  Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve']\n",
      "\n",
      "Retrieved contexts for the second prompt: ['[time: 1.7600000000000002-6.76]  Clarifying target audience and user roles. This document clarifies the target audience and user\\n[time: 6.76-13.4]  roles for Red Hat Rack project focusing on the distinction between end users and builders.\\n[time: 14.18-18.58]  End users vs. builders. End users consume the final product.\\n[time: 19.42-26.2]  Interact with a chat GPT-like application. Builders create and configure the AI systems\\n[time: 26.2-33.24]  used by end users. Example. Configure a Rack backend tweaking parameters for a specific\\n[time: 33.24-39.46]  experience such as chat GPT. We are targeting builders not end users. Builders optimize\\n[time: 39.46-49.18]  their systems for their specific end users. Builder archetypes. High-coder builders aka\\n[time: 49.18-55.92]  ProCode prefer SDKs and code-based solutions. They need access to all configurable\\n[time: 55.92-64.04]  parameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\n[time: 64.04-72.66]  system via a UI. Example. LamaStackCliMyRackApp.py-web.\\n[time: 75.56-83.84]  Low-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\n[time: 83.84-90.8]  systems. They could benefit from tools like the existing LamaStackPlayground and so on.\\n[time: 92.76-103.04]  Builders vs. Platformers vs. Opsers. Builders, AI engineers and AI devs. They use the platform\\n[time: 103.04-109.62]  and its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\n[time: 109.62-118.86]  whether they are considered AI engineers or AI devs. Platformers. AI platform engineers.\\n[time: 119.04-125.7]  Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve', '[time: 1.9200000000000008-8.58]  Key market use cases. RAC is being adopted across various industries for diverse applications,\\n[time: 8.86-15.22]  including knowledge question answering, providing accurate answers in customer service using product\\n[time: 15.22-21.24]  manuals or fax. Code generation, retrieving relevant code snippets and documentation to\\n[time: 21.24-28.04]  assist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\n[time: 28.04-35.06]  context. Customer service, improving support accuracy with access to current product information.\\n[time: 35.74-41.44]  Personal assistance, enabling more comprehensive and accurate information from AI assistants.\\n[time: 42.58-48.36]  Multi-hub question answering, handling complex multi-step questions through iterative retrieval.\\n[time: 49.3-54.6]  Legal applications, retrieving legal documents and case law for reliable legal opinions.\\n[time: 54.6-61.9]  General task assistance, aiding users in various tasks requiring information access and decision\\n[time: 61.9-68.08]  making. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\n[time: 68.08-75.48]  is also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\n[time: 75.48-75.66]  Technology re- warnedawanese, Independent benefits and Hispanic participants for\\n[time: 75.66-75.66] \\n[time: 75.66-75.76]  questions.', '[time: 125.7-133.44]  both builders for development and opsors for development slash operations, ensuring infrastructure\\n[time: 133.44-138.16]  is reliable, scalable, and supports self-service.\\n[time: 141.5-150.32]  Opsers. AI ML Ops engineers. Opsers focus on operationalizing and automating the AI ML lifecycle.\\n[time: 150.84-161.46]  For example, they use platform APIs to deploy, monitor, and manage models, enabling builders models to reach and succeed in production.\\n[time: 161.46-168.58]  Opsers work closely with platformers to ensure infrastructure meets operational needs.\\n[time: 172.08-180.2]  In summary, platformers enable builders and other developers to create systems for end users.\\n[time: 180.2-190.34]  The Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to build the best experience for their end users.\\n[time: 190.34-190.48]  of Short Wars.\\n[time: 190.56-191.16]  of Gutenberg\\n[time: 191.16-191.16] \\n[time: 191.16-191.16] \\n[time: 191.16-191.16] \\n[time: 191.16-191.16] \\n[time: 191.14-191.14]', \"[time: 108.42-117.48]  For developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\n[time: 117.48-121.24]  Data sovereignty and privacy\\n[time: 121.24-131.32]  Using confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\n[time: 131.32-142.92]  RAC offers a solution to privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\n[time: 143.06-150.32]  RAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\n[time: 151.08-157.8]  That is, certain users can access certain information based on their security clearance levels.\\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.8] \\n[time: 157.8-157.94]  It's another region, but from the difference in the reactivity field, the electrical data – Leo Meto is reaching to the Maininary leaf host.\\n[time: 157.94-157.94]\", \"[time: 1.1000000000000003-3.02]  The benefits of RAC\\n[time: 3.02-12.46]  The retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\n[time: 12.78-21.04]  Grounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\n[time: 21.840000000000003-22.0]  Accuracy\\n[time: 22.6-27.44]  RAC provides an LLM with sources it can cite so users can verify these claims.\\n[time: 27.44-34.5]  You can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\n[time: 35.1-43.42]  Overall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\n[time: 44.6-45.34]  Cost effectiveness\\n[time: 45.34-56.2]  Retraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\n[time: 56.2-66.58]  With RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\n[time: 67.08-69.96]  RAC can also reduce inference codes.\\n[time: 69.96-83.7]  LLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\n[time: 83.7-96.8]  Rather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\n[time: 97.34-99.2]  Developer Control\\n[time: 99.83999999999999-108.12]  Compared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness, \n",
    "    ResponseRelevancy,\n",
    ") \n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [line.strip() for line in reference_for_first_prompt.strip().split('\\n')]\n",
    "reference_list_for_second_prompt = [line.strip() for line in reference_for_second_prompt.strip().split('\\n')]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\")\n",
    "print(f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 40.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'List RAG key market use cases': 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 41.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'Name Red Hat RAG target audience and customers': 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 12.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 12.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 12.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'List RAG key market use cases': 0.9039218462290007\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_first_prompt,\n",
    "        response=response_for_first_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    "    )\n",
    "response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "response_relevancy_score_for_first_prompt = await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'Name Red Hat RAG target audience and customers': 0.9862592839954273\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "        user_input=user_input_for_second_prompt,\n",
    "        response=response_for_second_prompt,\n",
    "        retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    "    )\n",
    "response_relevancy_score_for_second_prompt = await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    "print(f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
